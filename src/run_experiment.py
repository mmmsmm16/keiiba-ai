import argparse
import yaml
import pandas as pd
import numpy as np
# lightgbm moved down
import os
import sys
import logging
import pickle
import json
import subprocess
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from sklearn.metrics import roc_auc_score, log_loss, ndcg_score, average_precision_score, brier_score_loss
from sklearn.model_selection import KFold
import mlflow

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.preprocessing.feature_pipeline import FeaturePipeline
from src.utils.leak_detector import check_data_leakage
from src.preprocessing.loader import JraVanDataLoader
from src.preprocessing.cleansing import DataCleanser
from src.preprocessing.dataset import DatasetSplitter
from src.config.validator import ConfigValidator
import lightgbm as lgb

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_config(config_path: str) -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«(YAML)ã‚’èª­ã¿è¾¼ã‚€"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def run_experiment(config_path: str, strict: bool = False):
    """å®Ÿé¨“ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    # 1. è¨­å®šã®èª­ã¿è¾¼ã¿
    try:
        config = load_config(config_path)
    except Exception as e:
        logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {config_path}. ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # [Enhanced] Config Guardrail Check
    # Check if strict mode is enabled in config or args
    config_strict = config.get('strict', False)
    is_strict = strict or config_strict
    
    try:
        ConfigValidator.validate(config, config_path=config_path, strict=is_strict)
    except ValueError as e:
        logger.error(f"â›” Config Validation Failed: {e}")
        # Stop experiment immediately
        return

    exp_name = config.get('experiment_name', f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    feature_blocks = config.get('features', [])
    model_params = config.get('model_params', {})
    dataset_cfg = config.get('dataset', {})
    calibration_cfg = config.get('calibration', {})
    
    # æˆæžœç‰©ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    artifact_dir = f"models/experiments/{exp_name}"
    os.makedirs(artifact_dir, exist_ok=True)
    
    # è¨­å®šã®ã‚³ãƒ”ãƒ¼ã‚’ä¿å­˜
    with open(os.path.join(artifact_dir, 'config.yaml'), 'w') as f:
        yaml.dump(config, f)
        
    # [NEW] Git Metadata retrieval
    git_hash = "unknown"
    is_dirty = False
    try:
        git_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'], stderr=subprocess.DEVNULL).decode('ascii').strip()
        status = subprocess.check_output(['git', 'status', '--porcelain'], stderr=subprocess.DEVNULL).decode('ascii').strip()
        is_dirty = bool(status)
    except Exception:
        pass
        
    # [NEW] Save Metadata for Leaderboard/Reproducibility
    metadata = {
        'experiment_name': exp_name,
        'model_type': model_params.get('model_type', 'lightgbm'),
        'objective': model_params.get('objective', ''),
        'target_col': dataset_cfg.get('target_col', ''),
        'binary_target': dataset_cfg.get('binary_target', ''),
        'metrics': model_params.get('metric', []),
        'time_decay_enabled': config.get('sample_weight', {}).get('enabled', False),
        'time_decay_strategy': config.get('sample_weight', {}).get('strategy', 'none'),
        'valid_year': dataset_cfg.get('valid_year', 2024),
        'train_end_date': dataset_cfg.get('train_end_date', ''),
        'feature_count': len(feature_blocks),
        'calibration_enabled': calibration_cfg.get('enabled', False),
        'timestamp': datetime.now().isoformat(),
        'git': {
            'commit_hash': git_hash,
            'is_dirty': is_dirty
        },
        'strict_mode': is_strict
    }
    with open(os.path.join(artifact_dir, 'metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=4)
        
    logger.info(f"ðŸš€ å®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™: {exp_name}")
    logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡ãƒ–ãƒ­ãƒƒã‚¯: {feature_blocks}")

    # [Optim] Data Loading OUTSIDE MLFlow context to prevent deadlock
    loader = JraVanDataLoader()
    start_date = dataset_cfg.get('train_start_date', '2015-01-01')
    end_date = dataset_cfg.get('test_end_date', '2025-12-31')
    jra_only = dataset_cfg.get('jra_only', False)
    skip_odds = dataset_cfg.get('drop_market_data', False)
    
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­ ({start_date} ~ {end_date})...")
    # [Optim] skip_training=True because no features use jvd_hc (prevents OOM)
    raw_df = loader.load(history_start_date=start_date, end_date=end_date, jra_only=jra_only, skip_odds=skip_odds, skip_training=True)
    
    cleanser = DataCleanser()
    clean_df = cleanser.cleanse(raw_df)

    with mlflow.start_run(run_name=exp_name):
        logger.info(f"âœ¨ MLFlow Run Started: {exp_name}")
        
        # 3. ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        logger.info("FeaturePipelineã‚’åˆæœŸåŒ–ä¸­...")
        try:
            pipeline = FeaturePipeline(cache_dir="data/features")
            logger.info(f"FeaturePipelineåˆæœŸåŒ–å®Œäº†. ãƒ–ãƒ­ãƒƒã‚¯æ•°: {len(feature_blocks)}")
            sys.stdout.flush()
            
            logger.info("FeaturePipelineã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ä¸­ (load_features)...")
            df = pipeline.load_features(clean_df, feature_blocks)
            logger.info(f"ç‰¹å¾´é‡ä½œæˆå®Œäº†: Shape={df.shape}")
            sys.stdout.flush()
        except Exception as e:
            logger.error(f"âŒ FeaturePipelineã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
            return
        
        # 4. ãƒªãƒ¼ã‚¯æ¤œçŸ¥ & ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ
        logger.info("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆã¨ãƒªãƒ¼ã‚¯æ¤œçŸ¥ã‚’å®Ÿè¡Œä¸­...")
        if 'rank' in clean_df.columns and 'target' not in df.columns:
            if 'rank' not in df.columns:
                target_source = clean_df[['race_id', 'horse_number', 'rank', 'odds']]
                df = pd.merge(df, target_source, on=['race_id', 'horse_number'], how='left')

            def create_ranking_target(rank):
                if pd.isna(rank): return 0
                if rank == 1: return 3
                elif rank == 2: return 2
                elif rank == 3: return 1
                else: return 0
            df['target'] = df['rank'].apply(create_ranking_target)
            
            if model_params.get('objective') == 'regression':
                logger.info("ðŸŽ¯ å›žå¸°ãƒ¢ãƒ¼ãƒ‰: Target = FinalOdds * (Rank==1) ã‚’ä½œæˆã—ã¾ã™ã€‚")
                df['target'] = df.apply(lambda row: row['odds'] if row['rank'] == 1 else 0.0, axis=1).fillna(0.0)
                
        try:
            check_data_leakage(df, target_col='target')
        except ValueError as e:
            logger.error(f"â›” ãƒªãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {e}")
            return

        # 5. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        splitter = DatasetSplitter()
        valid_year = dataset_cfg.get('valid_year', 2024)
        train_end_str = dataset_cfg.get('train_end_date', '2023-12-31')
        train_end_dt = pd.to_datetime(train_end_str)
        train_end_year = train_end_dt.year
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²ã‚’å®Ÿè¡Œä¸­ (æ¤œè¨¼å¹´: {valid_year}, Train End: {train_end_year})...")
        
        key_cols = ['race_id', 'date', 'horse_id'] 
        for k in key_cols:
            if k not in df.columns and k in clean_df.columns:
                df[k] = clean_df[k]
                
        if 'year' not in df.columns and 'date' in df.columns:
            df['year'] = pd.to_datetime(df['date']).dt.year
        elif 'year' not in df.columns and 'year' in clean_df.columns:
            df['year'] = clean_df['year']

        datasets = splitter.split_and_create_dataset(df, valid_year=valid_year)
    train_set, valid_set = datasets['train'], datasets['valid']
    
    # [NEW] Time-Decay Weighting
    sample_weight_cfg = config.get('sample_weight', {})
    if sample_weight_cfg.get('enabled', False) and 'train' in datasets:
        strategy = sample_weight_cfg.get('strategy', 'time_decay')
        normalize = sample_weight_cfg.get('normalize', True)
        logger.info(f"âš–ï¸ Sample Weighting enabled. Strategy: {strategy}, Normalize: {normalize}")
        
        X_train = train_set['X']
        if 'date' in X_train.columns:
            dates = pd.to_datetime(X_train['date'])
        elif 'date' in df.columns:
            dates = pd.to_datetime(df.loc[X_train.index, 'date'])
        else:
            dates = None

        if dates is not None:
            years = dates.dt.year
            weights = np.ones(len(dates))
            
            if strategy == 'exponential':
                decay_rate = sample_weight_cfg.get('decay_rate', 0.001)
                # days_old = (train_end - race_date).days
                # æœªæ¥ã®æ—¥ä»˜ãŒå«ã¾ã‚Œã‚‹å ´åˆ (è¨­å®šãƒŸã‚¹ç­‰) ã¯0 (é‡ã¿1.0) ã«ã™ã‚‹
                days_old = (train_end_dt - dates).dt.days
                days_old = np.maximum(days_old, 0) 
                weights = np.exp(-decay_rate * days_old)
                
            elif strategy == 'piecewise':
                # delta = train_end_year - race_year
                # delta=0 (Last Year) -> 1.0
                # delta=1 (Prev Year) -> 0.7
                # delta=2 -> 0.5
                # else -> 0.3
                
                # Configã‹ã‚‰ç›¸å¯¾ãƒžãƒƒãƒ—ã‚’å–å¾— (ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
                # config format expectation:
                # year_weights: {0: 1.0, 1: 0.7, 2: 0.5, "default": 0.3}
                # äº’æ›æ€§ã®ãŸã‚ã€æ—¢å­˜ã®å¹´æŒ‡å®š(2024: 1.0)ãŒã‚ã‚‹å ´åˆã®å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚‚å…¥ã‚Œã‚‹ï¼Ÿ
                # ã„ã‚„ã€M4ã‹ã‚‰ã¯æ–°ã—ã„ç›¸å¯¾ãƒ­ã‚¸ãƒƒã‚¯ã§è¡Œãã€‚Configå´ã‚‚åˆã‚ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€
                # M3ã®Configã¯å¹´æŒ‡å®šã ã£ãŸã€‚
                # ã“ã“ã§ã¯ã€Œã‚­ãƒ¼ãŒ2000ä»¥ä¸Šã®å ´åˆã¯çµ¶å¯¾å¹´ã€ãã‚Œä»¥å¤–ã¯ç›¸å¯¾å¹´ã€ã¨åˆ¤å®šã—ã¦äº’æ›æ€§ã‚’ç¶­æŒã™ã‚‹ã€‚
                
                yw_cfg = sample_weight_cfg.get('year_weights', {})
                default_w = yw_cfg.get('default', 0.3)
                
                delta_years = train_end_year - years
                
                # ãƒ™ã‚¯ãƒˆãƒ«åŒ–é©ç”¨
                # ã¾ãšãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
                weights = np.full(len(X_train), default_w)
                
                # ã‚­ãƒ¼ã”ã¨ã«é©ç”¨
                for k, w in yw_cfg.items():
                    if k == 'default': continue
                    try:
                        k_int = int(k)
                        # 2000ä»¥ä¸Šãªã‚‰çµ¶å¯¾å¹´ã€ãã‚Œä»¥å¤–ãªã‚‰ç›¸å¯¾å¹´
                        if k_int > 1900:
                            # Absolute Year Mode (Legacy Support)
                            mask = (years == k_int)
                        else:
                            # Relative Year Mode (Delta)
                            mask = (delta_years == k_int)
                            
                        weights[mask] = w
                    except:
                        continue
                        
            if normalize:
                weights = weights / weights.mean()
                
            # Log stats
            w_series = pd.Series(weights)
            logger.info(f"  Weights stats: min={w_series.min():.4f}, max={w_series.max():.4f}, mean={w_series.mean():.4f}")
            
            train_set['weight'] = weights
            logger.info(f"  Weights: Min={weights.min():.4f}, Max={weights.max():.4f}, Mean={weights.mean():.4f}")

    # ç‰¹å¾´é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if skip_odds:
        logger.info("ðŸš« å¸‚å ´ãƒ‡ãƒ¼ã‚¿ (odds, popularity) ã‚’ç‰¹å¾´é‡ã‹ã‚‰é™¤å¤–ã—ã¾ã™ã€‚")
        market_cols = [c for c in train_set['X'].columns if any(m in c for m in ['odds', 'popularity'])]
        if market_cols:
            train_set['X'] = train_set['X'].drop(columns=market_cols)
            valid_set['X'] = valid_set['X'].drop(columns=market_cols)

    # [FIX] Exclude Features defined in config
    exclude_cols = dataset_cfg.get('exclude_features', [])
    if exclude_cols:
        logger.info(f"ðŸš« æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡ã‚’é™¤å¤–ã—ã¾ã™: {len(exclude_cols)} items")
        # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿
        drop_target = [c for c in exclude_cols if c in train_set['X'].columns]
        if drop_target:
            train_set['X'] = train_set['X'].drop(columns=drop_target)
            valid_set['X'] = valid_set['X'].drop(columns=drop_target)

    # 6. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model_type = model_params.get('model_type', 'lightgbm')
    objective = model_params.get('objective', 'lambdarank')
    do_calibration = calibration_cfg.get('enabled', False) and objective == 'binary'
    
    cat_features = dataset_cfg.get('categorical_features', [])
    auto_cat = [c for c in train_set['X'].columns if train_set['X'][c].dtype == 'object']
    cat_features = list(set(cat_features + auto_cat))
    cat_features = [c for c in cat_features if c not in ['race_id', 'date', 'horse_id', 'target', 'year', 'y', 'rank', 'odds', 'target_win', 'target_top3']]
    if cat_features:
        logger.info(f"Categorical features detected: {cat_features}")

    train_y, valid_y = train_set['y'], valid_set['y']
    valid_y_relevance = valid_y.copy()
    
    if objective == 'binary':
        binary_target = dataset_cfg.get('binary_target', 'top3')
        if binary_target == 'win':
            train_y, valid_y = (train_y == 3).astype(int), (valid_y == 3).astype(int)
        elif binary_target == 'top2':
            train_y, valid_y = (train_y >= 2).astype(int), (valid_y >= 2).astype(int)
        else:
            train_y, valid_y = (train_y > 0).astype(int), (valid_y > 0).astype(int)

    # ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã®ç¢ºå®š
    drop_cols = ['race_id', 'horse_id', 'date', 'target', 'year', 'y', 'rank', 'odds', 'target_win', 'target_top3', 'is_win', 'is_top3']
    feature_cols = [c for c in train_set['X'].columns if c not in drop_cols]
    cat_features = [c for c in cat_features if c in feature_cols]
    
    def prepare_df(df_input):
        df_out = df_input[feature_cols].copy()
        if model_type == 'catboost':
            for col in cat_features: df_out[col] = df_out[col].fillna("missing").astype(str)
        else:
            for col in cat_features: df_out[col] = df_out[col].astype('category')
        return df_out

    # å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã®äº‹å‰åŠ å·¥ (é«˜é€ŸåŒ–ã®ãŸã‚)
    logger.info("â³ ç‰¹å¾´é‡ã®äº‹å‰åŠ å·¥ä¸­...")
    X_train_processed = prepare_df(train_set['X'])
    X_valid_processed = prepare_df(valid_set['X'])

    def train_model(X_train_pre, t_y, t_group, valid_pts_pre, override_params=None, weight=None):
        params = override_params if override_params else model_params
        
        if model_type == 'catboost':
            import catboost as cb
            cb_params = params.copy()
            cb_params.pop('model_type', None); cb_params.pop('objective', None); cb_params.pop('early_stopping_rounds', None)
            loss_fn = 'Logloss' if objective == 'binary' else objective
            if objective == 'lambdarank': loss_fn = 'YetiRank'
            elif objective == 'regression': loss_fn = 'RMSE'
            
            t_pool = cb.Pool(data=X_train_pre, label=t_y, weight=weight, group_id=np.repeat(np.arange(len(t_group)), t_group) if objective == 'lambdarank' else None, cat_features=cat_features)
            eval_sets = []
            for vX, vy, vg in valid_pts_pre:
                v_pool = cb.Pool(data=vX, label=vy, group_id=np.repeat(np.arange(len(vg)), vg) if objective == 'lambdarank' else None, cat_features=cat_features)
                eval_sets.append(v_pool)
            
            fit_m = cb.CatBoostClassifier(loss_function=loss_fn, **cb_params) if objective == 'binary' else \
                    (cb.CatBoostRanker(loss_function=loss_fn, **cb_params) if objective == 'lambdarank' else \
                     cb.CatBoostRegressor(loss_function=loss_fn, **cb_params))
            fit_m.fit(t_pool, eval_set=eval_sets if eval_sets else None, early_stopping_rounds=params.get('early_stopping_rounds', 50), verbose=False)
            return fit_m
        else:
            lgb_train = lgb.Dataset(X_train_pre, label=t_y, categorical_feature=cat_features, free_raw_data=False, weight=weight)
            if objective == 'lambdarank': lgb_train.set_group(t_group)
            
            v_sets, v_names = [lgb_train], ['train']
            for i, (vX, vy, vg) in enumerate(valid_pts_pre):
                v_ds = lgb.Dataset(vX, label=vy, reference=lgb_train, categorical_feature=cat_features, free_raw_data=False)
                if objective == 'lambdarank': v_ds.set_group(vg)
                v_sets.append(v_ds)
                v_names.append(f'valid_{i}')
            
            l_params = params.copy(); l_params.pop('model_type', None)
            n_rounds = params.get('num_boost_round', params.get('n_estimators', 100))
            callbacks = [lgb.log_evaluation(period=0)]
            if valid_pts_pre: callbacks.append(lgb.early_stopping(stopping_rounds=50))
            return lgb.train(l_params, lgb_train, num_boost_round=n_rounds, valid_sets=v_sets, valid_names=v_names, callbacks=callbacks)

    # 5.5 Dry Run / Sanity Check
    logger.info("ðŸ› ï¸ å­¦ç¿’å‰ã®ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ (Dry Run) ã‚’å®Ÿè¡Œä¸­...")
    try:
        _ = train_model(X_train_processed.head(100), train_y[:100], [100], [], override_params={'n_estimators': 1})
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯åˆæ ¼ã€‚")
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return

    # ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model_save_path_pkl = os.path.join(artifact_dir, 'model.pkl')
    model_save_path_cbm = os.path.join(artifact_dir, 'model.cbm')
    model = None

    if model_type == 'catboost' and os.path.exists(model_save_path_cbm):
        import catboost as cb
        logger.info(f"ðŸ“¦ æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™: {model_save_path_cbm}")
        model = cb.CatBoostClassifier() if objective == 'binary' else \
                (cb.CatBoostRanker() if objective == 'lambdarank' else cb.CatBoostRegressor())
        model.load_model(model_save_path_cbm)
    elif model_type != 'catboost' and os.path.exists(model_save_path_pkl):
        logger.info(f"ðŸ“¦ æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™: {model_save_path_pkl}")
        with open(model_save_path_pkl, 'rb') as f: model = pickle.load(f)

    if model is None:
        logger.info(f"{model_type.upper()}ã®å­¦ç¿’ã‚’é–‹å§‹ (ç›®çš„é–¢æ•°: {objective})...")
        model = train_model(X_train_processed, train_y, train_set['group'], [(X_valid_processed, valid_y, valid_set['group'])], weight=train_set.get('weight'))
        # ä¸­é–“ä¿å­˜
        if model_type == 'catboost': model.save_model(model_save_path_cbm)
        else:
            with open(model_save_path_pkl, 'wb') as f: pickle.dump(model, f)
        logger.info(f"âœ… ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    
    # ç¢ºçŽ‡æ ¡æ­£å™¨ã®å­¦ç¿’ (OOF)
    calibrator = None
    calibrator_path = os.path.join(artifact_dir, 'calibrator.pkl')
    oof_path = os.path.join(artifact_dir, 'oof_probs.npy')
    
    if do_calibration:
        from src.models.calibration import ProbabilityCalibrator
        if os.path.exists(calibrator_path):
            logger.info(f"ðŸ“¦ æ—¢å­˜ã®ç¢ºçŽ‡æ ¡æ­£å™¨ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™: {calibrator_path}")
            with open(calibrator_path, 'rb') as f: calibrator = pickle.load(f)
        else:
            n_folds, method = calibration_cfg.get('n_folds', 5), calibration_cfg.get('method', 'platt')
            
            if os.path.exists(oof_path):
                logger.info(f"ðŸ’¾ æ—¢å­˜ã®OOFäºˆæ¸¬ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™: {oof_path}")
                oof_probs = np.load(oof_path)
            else:
                logger.info(f"ðŸ”® ç¢ºçŽ‡æ ¡æ­£å™¨ã®å­¦ç¿’ã‚’é–‹å§‹ (Method: {method}, OOF Folds: {n_folds})...")
                
                # OOFåˆ†å‰²ç”¨ã«race_idã‚’ç¢ºä¿
                if 'race_id' not in train_set['X'].columns:
                    train_set['X']['race_id'] = df.loc[train_set['X'].index, 'race_id'].values
                
                unique_races = train_set['X']['race_id'].unique()
                kf, oof_probs = KFold(n_splits=n_folds, shuffle=True, random_state=42), np.zeros(len(train_y))
                
                for fold, (train_idx, val_idx) in enumerate(kf.split(unique_races)):
                    logger.info(f"  Calibration OOF Fold {fold+1}/{n_folds} training...")
                    mask_t = train_set['X']['race_id'].isin(unique_races[train_idx])
                    mask_v = train_set['X']['race_id'].isin(unique_races[val_idx])
                    
                    X_t_pre, y_t = X_train_processed[mask_t], train_y[mask_t]
                    X_v_pre, y_v = X_train_processed[mask_v], train_y[mask_v]
                    
                    def get_groups(mask): return train_set['X'][mask].groupby('race_id', sort=False).size().values
                    
                    oof_params = model_params.copy()
                    if oof_params.get('n_estimators', 0) > 500: oof_params['n_estimators'] = 500
                    # OOFå­¦ç¿’ã§ã¯ early_stopping ã‚’ç„¡åŠ¹åŒ–ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
                    oof_params.pop('early_stopping_rounds', None)
                    
                    try:
                        m_oof = train_model(X_t_pre, y_t, get_groups(mask_t), [], override_params=oof_params)
                        logger.info(f"  Calibration OOF Fold {fold+1}/{n_folds} predicting...")
                        probs = m_oof.predict(X_v_pre)
                        # Nan Clean
                        probs = np.nan_to_num(probs, nan=0.0)
                        oof_probs[mask_v] = probs
                    except Exception as e:
                        logger.error(f"  Fold {fold+1} failed: {e}")
                        # Fallback: fill with mean
                        oof_probs[mask_v] = y_t.mean()
                
                np.save(oof_path, oof_probs)
                
            calibrator = ProbabilityCalibrator(method=method)
            try:
                # Pre-clean OOF
                oof_clean = np.nan_to_num(oof_probs, nan=0.0)
                oof_clean = np.clip(oof_clean, 0.0, 1.0)
                calibrator.fit(train_y.values, oof_clean)
                logger.info("âœ… ç¢ºçŽ‡æ ¡æ­£å™¨ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            except Exception as e:
                logger.warning(f"âš ï¸ Isotonic Calibration failed: {e}. Falling back to Sigmoid (Platt).")
                try:
                    calibrator = ProbabilityCalibrator(method='sigmoid')
                    calibrator.fit(train_y.values, oof_clean)
                    logger.info("âœ… Sigmoidæ ¡æ­£å™¨ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸ (Fallback)ã€‚")
                except Exception as e2:
                     logger.error(f"âŒ Calibration è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e2}. æ ¡æ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                     calibrator = None

            if calibrator:
                with open(calibrator_path, 'wb') as f: pickle.dump(calibrator, f)
    
    # 7. è©•ä¾¡
    logger.info("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
    preds = model.predict(prepare_df(valid_set['X']))
    if calibrator:
        logger.info("  Applying probability calibration to predictions...")
        preds = calibrator.predict(preds)
        
    binary_y = (valid_y > 0).astype(int) 
    if objective == 'regression':
        from sklearn.metrics import mean_squared_error
        auc_score, ll_score, bs_score, ap_score = 0.0, np.sqrt(mean_squared_error(valid_y, preds)), 0.0, 0.0
        logger.info(f"Calculated RMSE: {ll_score:.4f}")
    else:
        auc_score = roc_auc_score(binary_y, preds) if len(np.unique(binary_y)) > 1 else 0.0
        ll_score = log_loss(binary_y, preds) if len(np.unique(binary_y)) > 1 else 0.0
        ap_score = average_precision_score(binary_y, preds) if len(np.unique(binary_y)) > 1 else 0.0
        bs_score = brier_score_loss(binary_y, preds) if len(np.unique(binary_y)) > 1 else 0.0
    
    # 7. è©•ä¾¡
    logger.info("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
    X_valid = valid_set['X']
    
    # è©•ä¾¡ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾— (Distance, N_Horses)
    # df or clean_df ã‹ã‚‰å–å¾—ã€‚Indexã¯ä¸€è‡´ã—ã¦ã„ã‚‹å‰æ
    # X_validã®indexã‚’ä½¿ã£ã¦clean_dfã‹ã‚‰å–å¾—ã™ã‚‹
    meta_df = clean_df.loc[X_valid.index, ['race_id', 'distance']].copy()
    
    # é ­æ•°(n_horses)ã‚’è¨ˆç®— (race_idã”ã¨ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°)
    # â€» clean_dfã«ã¯å…¨é ­ã„ã‚‹ã¯ãš
    race_counts = meta_df.groupby('race_id')['race_id'].transform('count')
    meta_df['n_horses'] = race_counts

    preds = model.predict(prepare_df(X_valid))
    if calibrator:
        logger.info("  Applying probability calibration to predictions...")
        preds = calibrator.predict(preds)
        
    binary_y = (valid_y > 0).astype(int) 
    
    # Metrics Calculation Helper
    def calc_ranking_metrics(y_true_bin, y_score, groups):
        ndcg_list, recall_list = [], []
        curr = 0
        for size in groups:
            # sizeãŒä¸€è‡´ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ— (å¿µã®ãŸã‚)
            if curr + size > len(y_true_bin): break
            
            y_t_bin = y_true_bin[curr : curr + size]
            y_s = y_score[curr : curr + size]
            
            if size > 1 and np.sum(y_t_bin) > 0:
                # NDCG (ranking quality)
                # binary target for NDCG implies relevance 1 or 0
                ndcg_list.append(ndcg_score([y_t_bin], [y_s], k=5))
                
                # Recall@5 (Race-Hit@5: Top3é¦¬ãŒäºˆæ¸¬Top5ã«1é ­ã§ã‚‚å…¥ã‚Œã°1)
                top_k_idx = np.argsort(y_s)[::-1][:5]
                # Hit count in Top 5
                hits = np.sum(y_t_bin[top_k_idx])
                # Recall per race definition: User definition seems to be "Any Hit" (based on previous chat) 
                # OR standard Recall (Hits / Total Positives).
                # User prompted: "Race-Hit@5 (Top5ã«Top3é¦¬ãŒ1é ­ã§ã‚‚å«ã¾ã‚Œã‚‹ã‹)" -> This is Hit Rate @ 5.
                # However, M4-A Audit used standard Recall formula? 
                # Wait, User said: "Verify Recall@5 ... (0.9635)". 0.96 is extremely high for "All Top3 found in Top5".
                # It MUST be "Any Hit".
                # Let's calculate both: "Race-Hit" (Any) and "Recall" (Coverage).
                # But for the report, User specifically asked for "Race-Hit@5" AND "NDCG".
                # I will calculate "RaceHit@5" (Variable name recall_at_5 in legacy code likely meant this).
                
                is_hit = 1.0 if hits > 0 else 0.0
                recall_list.append(is_hit)
                
            curr += size
        
        return np.mean(ndcg_list) if ndcg_list else 0.0, np.mean(recall_list) if recall_list else 0.0

    # Overall Metrics
    groups_valid = valid_set['group']
    ndcg_all, hit_all = calc_ranking_metrics(binary_y.values, preds, groups_valid)
    
    if objective == 'regression':
         from sklearn.metrics import mean_squared_error
         auc_score, ll_score, bs_score, ap_score = 0.0, np.sqrt(mean_squared_error(valid_y, preds)), 0.0, 0.0
         logger.info(f"Calculated RMSE: {ll_score:.4f}")
    else:
         auc_score = roc_auc_score(binary_y, preds) if len(np.unique(binary_y)) > 1 else 0.0
         ll_score = log_loss(binary_y, preds) if len(np.unique(binary_y)) > 1 else 0.0
         ap_score = average_precision_score(binary_y, preds) if len(np.unique(binary_y)) > 1 else 0.0
         bs_score = brier_score_loss(binary_y, preds) if len(np.unique(binary_y)) > 1 else 0.0

    logger.info(f"Overall - NDCG@5: {ndcg_all:.4f}, Race-Hit@5: {hit_all:.4f}")

    # Segment Evaluation
    # Re-construct dataframe for segmentation
    eval_df = pd.DataFrame({
        'race_id': meta_df['race_id'].values,
        'distance': meta_df['distance'].values,
        'n_horses': meta_df['n_horses'].values,
        'y_bin': binary_y.values,
        'y_score': preds
    })
    
    # Segment: Small Field (<= 10)
    # race_idå˜ä½ã§groupã—ã¦åˆ¤å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€eval_dfã¯æ—¢ã«å±•é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚
    # race_idã”ã¨ã«é›†ç´„ã—ã¦metricsè¨ˆç®—ã™ã‚‹ã®ã¯é‡ã„ã®ã§ã€groupsã¨maskã‚’ä½¿ã£ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
    
    # 1. Race Level Attributes Map
    race_attrs = eval_df.groupby('race_id')[['distance', 'n_horses']].first()
    
    # 2. Filter Race IDs
    small_races = race_attrs[race_attrs['n_horses'] <= 10].index
    mile_races = race_attrs[(race_attrs['distance'] >= 1400) & (race_attrs['distance'] <= 1800)].index
    
    def eval_subset(subset_races, label):
        # subsetã«å«ã¾ã‚Œã‚‹è¡Œã®ã¿æŠ½å‡º
        # Note: groupæ§‹é€ ã‚’ç¶­æŒã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
        # å˜ç´”ã«maskã™ã‚‹ã¨groupãŒå£Šã‚Œã‚‹ã€‚
        # Raceå˜ä½ã§ãƒ«ãƒ¼ãƒ—ã—ã¦ã€ãã®RaceãŒsubsetã«å«ã¾ã‚Œã‚‹ã‹åˆ¤å®šã™ã‚‹ã®ãŒç¢ºå®Ÿã€‚
        
        ndcg_list, hit_list = [], []
        curr = 0
        for size in groups_valid:
            # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã®race_idã‚’å–å¾— (å…ˆé ­1è¡Œã§ååˆ†)
            if curr >= len(eval_df): break
            rid = eval_df.iloc[curr]['race_id']
            
            if rid in subset_races:
                chunk = eval_df.iloc[curr : curr + size]
                y_t_bin = chunk['y_bin'].values
                y_s = chunk['y_score'].values
                
                if size > 1 and np.sum(y_t_bin) > 0:
                    ndcg_list.append(ndcg_score([y_t_bin], [y_s], k=5))
                    top_k = np.argsort(y_s)[::-1][:5]
                    hits = np.sum(y_t_bin[top_k])
                    hit_list.append(1.0 if hits > 0 else 0.0)
            
            curr += size
        
        val_ndcg = np.mean(ndcg_list) if ndcg_list else 0.0
        val_hit = np.mean(hit_list) if hit_list else 0.0
        logger.info(f"Segment [{label}] - NDCG@5: {val_ndcg:.4f}, Race-Hit@5: {val_hit:.4f}")
        return val_ndcg, val_hit

    ndcg_small, hit_small = eval_subset(small_races, "SmallField<=10")
    ndcg_mile, hit_mile = eval_subset(mile_races, "Mile1400-1800")
    
    # Metrics JSON Save
    metrics_summary = {
        'overall': {
            'auc': auc_score, 'logloss': ll_score, 'ndcg_5': ndcg_all, 'race_hit_5': hit_all
        },
        'segments': {
            'small_field': {'ndcg_5': ndcg_small, 'race_hit_5': hit_small},
            'mile': {'ndcg_5': ndcg_mile, 'race_hit_5': hit_mile}
        }
    }
    with open(os.path.join(artifact_dir, 'metrics.json'), 'w') as f:
        json.dump(metrics_summary, f, indent=4)


    # 8. ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰è¨˜éŒ²
    leaderboard_path = "reports/experiment_leaderboard.md"
    if not os.path.exists(leaderboard_path):
        with open(leaderboard_path, 'w', encoding='utf-8') as f:
            f.write("| Exp ID | Features | Model | Year | AUC | LogLoss | Brier | PR-AUC | NDCG | Recall@5 | ROI | Desc |\n")
            f.write("| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n")
    row = f"| {exp_name} | {len(feature_blocks)} features | {objective} | {valid_year} | {auc_score:.4f} | {ll_score:.4f} | {bs_score:.4f} | {ap_score:.4f} | {ndcg_all:.4f} | {hit_all:.4f} | 0.0% | {dataset_cfg.get('description', '')} |\n"
    with open(leaderboard_path, 'a', encoding='utf-8') as f: f.write(row)
    logger.info("âœ… å®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Experiment with Config Guardrails")
    parser.add_argument('--config', type=str, required=True, help='Path to config yaml')
    parser.add_argument('--strict', action='store_true', help='Enable strict config validation (Warnings -> Errors)')
    args = parser.parse_args()
    
    run_experiment(args.config, strict=args.strict)

"""
Auto Predict v13 Market Residual
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ãƒ»Discordé€šçŸ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç‰¹å¾´:
- ãƒ¬ãƒ¼ã‚¹ç™ºèµ°10åˆ†å‰ã®ã‚ªãƒƒã‚ºã‚’ä½¿ç”¨
- delta_logit + p_market_snapshot ã§äºˆæ¸¬å†è¨ˆç®—
- ä¸‰é€£è¤‡ BOX4 æˆ¦ç•¥

Usage:
    docker compose exec app python src/scripts/auto_predict_v13.py
    docker compose exec app python src/scripts/auto_predict_v13.py --dry-run
    docker compose exec app python src/scripts/auto_predict_v13.py --date 2025-12-21
"""
import os
import sys
import json
import time
import requests
import argparse
import pandas as pd
import numpy as np
import logging
import lightgbm as lgb
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from scipy.special import expit, logit as scipy_logit
from itertools import combinations
from sqlalchemy import create_engine

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(os.path.join(os.path.dirname(__file__), '../../logs/auto_predict_v13.log'))
    ]
)
logger = logging.getLogger(__name__)

# å®šæ•°
STATE_FILE_PATH = os.path.join(os.path.dirname(__file__), '../../data/state/notified_races_v13.json')
MODEL_DIR = os.path.join(os.path.dirname(__file__), '../../models/v13_market_residual')

# å ´ã‚³ãƒ¼ãƒ‰
VENUE_MAP = {
    '01': 'æœ­å¹Œ', '02': 'å‡½é¤¨', '03': 'ç¦å³¶', '04': 'æ–°æ½Ÿ', '05': 'æ±äº¬',
    '06': 'ä¸­å±±', '07': 'ä¸­äº¬', '08': 'äº¬éƒ½', '09': 'é˜ªç¥', '10': 'å°å€‰'
}

def load_env_manual():
    """æ‰‹å‹•ã§.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        env_path = os.path.join(os.path.dirname(__file__), '../../.env')
        if os.path.exists(env_path):
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    if '=' in line:
                        key, val = line.split('=', 1)
                        os.environ[key.strip()] = val.strip()
    except Exception as e:
        logger.warning(f".envèª­ã¿è¾¼ã¿å¤±æ•—: {e}")

def get_db_engine():
    """DBæ¥ç¶šã‚¨ãƒ³ã‚¸ãƒ³ã‚’å–å¾—"""
    user = os.environ.get('POSTGRES_USER', 'postgres')
    password = os.environ.get('POSTGRES_PASSWORD', 'postgres')
    host = os.environ.get('POSTGRES_HOST', 'host.docker.internal')
    port = os.environ.get('POSTGRES_PORT', '5433')
    dbname = os.environ.get('POSTGRES_DB', 'pckeiba')
    return create_engine(f"postgresql://{user}:{password}@{host}:{port}/{dbname}")


class OddsFetcher:
    """æ™‚ç³»åˆ—ã‚ªãƒƒã‚ºå–å¾—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, engine):
        self.engine = engine
    
    def fetch_latest_odds(self, race_id: str, race_dt: datetime, before_minutes: int = 10) -> Optional[Dict[int, float]]:
        """
        æŒ‡å®šãƒ¬ãƒ¼ã‚¹ã®æœ€æ–°ã‚ªãƒƒã‚ºã‚’å–å¾— (ç™ºèµ°Nåˆ†å‰æ™‚ç‚¹)
        
        Args:
            race_id: YYYY+PP+KK+NN+RR å½¢å¼
            race_dt: ç™ºèµ°æ—¥æ™‚ (datetime)
            before_minutes: ç™ºèµ°ä½•åˆ†å‰ã¾ã§ã®ã‚ªãƒƒã‚ºã‚’å–å¾—ã™ã‚‹ã‹
        
        Returns:
            {horse_number: odds} ã®è¾æ›¸
        """
        # race_idã‹ã‚‰ã‚­ãƒ¼è¦ç´ ã‚’æŠ½å‡º
        kaisai_nen = race_id[:4]
        keibajo = race_id[4:6]
        kaisai_kai = race_id[6:8]
        kaisai_nichi = race_id[8:10]
        race_bango = race_id[10:12]
        
        # åŸºæº–æ™‚åˆ» (ç™ºèµ° - Nåˆ†)
        target_dt = race_dt - timedelta(minutes=before_minutes)
        # MMDDHHMM å½¢å¼ã«å¤‰æ› (apd_sokuho_o1.happyo_tsukihi_jifunã¨æ¯”è¼ƒç”¨)
        target_ts_str = target_dt.strftime('%m%d%H%M')
        
        query = f"""
        SELECT happyo_tsukihi_jifun, odds_tansho
        FROM apd_sokuho_o1
        WHERE kaisai_nen = '{kaisai_nen}'
          AND keibajo_code = '{keibajo}'
          AND kaisai_kai = '{kaisai_kai}'
          AND kaisai_nichime = '{kaisai_nichi}'
          AND race_bango = '{race_bango}'
          AND happyo_tsukihi_jifun <= '{target_ts_str}'
        ORDER BY happyo_tsukihi_jifun DESC
        LIMIT 1
        """
        
        try:
            df = pd.read_sql(query, self.engine)
            if df.empty:
                return None
            
            odds_str = df.iloc[0]['odds_tansho']
            return self._parse_odds_string(odds_str)
        except Exception as e:
            logger.error(f"ã‚ªãƒƒã‚ºå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _parse_odds_string(self, odds_str: str) -> Dict[int, float]:
        """
        ã‚ªãƒƒã‚ºæ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
        Format: 28é ­åˆ† Ã— 8æ–‡å­— (é¦¬ç•ª2 + ã‚ªãƒƒã‚º4 + äººæ°—2)
        """
        result = {}
        if not odds_str or len(odds_str) < 8:
            return result
        
        for i in range(28):  # æœ€å¤§28é ­
            start = i * 8
            if start + 8 > len(odds_str):
                break
            
            chunk = odds_str[start:start + 8]
            try:
                horse_num = int(chunk[0:2])
                odds_val = int(chunk[2:6]) / 10.0
                
                if horse_num > 0 and odds_val > 0:
                    result[horse_num] = odds_val
            except:
                continue
        
        return result


class V13Predictor:
    """v13 market_residual äºˆæ¸¬ã‚¯ãƒ©ã‚¹ (parquetå°‚ç”¨)
    
    é‹ç”¨æ™‚ã¯äº‹å‰ã«å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¦parquetã‚’æœ€æ–°åŒ–ã—ã¦ãã ã•ã„:
        docker compose exec app python src/preprocessing/run_preprocessing.py
    """
    
    def __init__(self):
        self.models = self._load_models()
        self.engine = get_db_engine()
        self.odds_fetcher = OddsFetcher(self.engine)
        
        # å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ (parquet) ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._preprocessed_cache = None
        self._parquet_path = os.path.join(os.path.dirname(__file__), '../../data/processed/preprocessed_data.parquet')
    
    def _load_models(self) -> List[lgb.Booster]:
        """v13ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        models = []
        for fold in ['2022', '2023', '2024']:
            path = os.path.join(MODEL_DIR, f'v13_fold_{fold}.txt')
            if os.path.exists(path):
                models.append(lgb.Booster(model_file=path))
                logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰: {path}")
        
        if not models:
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MODEL_DIR}")
        
        return models
    
    def _get_preprocessed_cache(self) -> pd.DataFrame:
        """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼ˆåˆå›ã®ã¿ãƒ­ãƒ¼ãƒ‰ï¼‰"""
        if self._preprocessed_cache is None:
            if os.path.exists(self._parquet_path):
                self._preprocessed_cache = pd.read_parquet(self._parquet_path)
                logger.info(f"å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {len(self._preprocessed_cache)} rows")
            else:
                raise FileNotFoundError(
                    f"å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self._parquet_path}\n"
                    "â†’ å…ˆã«å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\n"
                    "   docker compose exec app python src/preprocessing/run_preprocessing.py"
                )
        return self._preprocessed_cache
    
    def get_features(self, date_str: str, race_ids: List[str]) -> pd.DataFrame:
        """
        parquetã‹ã‚‰ç‰¹å¾´é‡ã‚’å–å¾—
        
        Args:
            date_str: YYYYMMDDå½¢å¼ã®æ—¥ä»˜
            race_ids: å¯¾è±¡ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆ
        
        Returns:
            ç‰¹å¾´é‡ä»˜ãDataFrame
        
        Raises:
            ValueError: parquetã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆ
        """
        cache = self._get_preprocessed_cache()
        
        # race_id ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦æ¤œç´¢
        cache['race_id_str'] = cache['race_id'].astype(str)
        race_ids_str = [str(rid) for rid in race_ids]
        
        result = cache[cache['race_id_str'].isin(race_ids_str)].copy()
        result = result.drop(columns=['race_id_str'], errors='ignore')
        
        found_ids = set(result['race_id'].astype(str).unique()) if not result.empty else set()
        missing_ids = [rid for rid in race_ids if str(rid) not in found_ids]
        
        if missing_ids:
            logger.warning(
                f"parquetã«ãƒ‡ãƒ¼ã‚¿ãªã—: {len(missing_ids)} races\n"
                f"  ä¸è¶³ãƒ¬ãƒ¼ã‚¹: {missing_ids[:5]}...\n"
                "â†’ å‰å‡¦ç†ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„:\n"
                "   docker compose exec app python src/preprocessing/run_preprocessing.py"
            )
        
        if not result.empty:
            logger.info(f"parquetã‹ã‚‰å–å¾—: {len(result)} rows, {len(found_ids)} races")
        
        return result
    
    def predict_race(self, race_df: pd.DataFrame, snapshot_odds: Dict[int, float]) -> pd.DataFrame:
        """
        ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬ã‚’å®Ÿè¡Œ (paper_trade_run.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
        
        Args:
            race_df: ç‰¹å¾´é‡ä»˜ããƒ‡ãƒ¼ã‚¿
            snapshot_odds: {horse_number: odds} æ™‚ç³»åˆ—ã‚ªãƒƒã‚º (T-10m)
        
        Returns:
            äºˆæ¸¬çµæœDataFrame (prob_residual_softmax, rank ä»˜ã)
        """
        df = race_df.copy()
        
        # === LEAK GUARD ===
        # æœªæ¥æƒ…å ±(rankç­‰)ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
        forbidden_cols = ['rank', 'rank_result', 'kakutei_chakujun', 'payout', 'time', 'agari']
        leaks = [c for c in forbidden_cols if c in df.columns]
        if leaks:
            raise ValueError(f"Leakage detected! Forbidden columns found in input: {leaks}")
        
        # === LEAK PREVENTION ===
        # snapshot oddsã‚’ãƒãƒ¼ã‚¸
        df['odds_snapshot'] = df['horse_number'].map(snapshot_odds)
        
        # Snapshot odds ã‹ã‚‰äººæ°—é †ã‚’è¨ˆç®—ã—ã¦ä¸Šæ›¸ã (Parquetã®ç¢ºå®šæƒ…å ±ã‚’éš è”½)
        if 'odds_snapshot' in df.columns and df['odds_snapshot'].notna().any():
            # ã‚ªãƒƒã‚ºæ˜‡é †ã§ãƒ©ãƒ³ã‚¯ä»˜ã‘ (æ¬ æã¯æœ€ä¸‹ä½æ‰±ã„)
            temp_odds = df['odds_snapshot'].fillna(float('inf'))
            # method='min'ã§åŒç‡ã¯åŒã˜é †ä½
            df['popularity'] = temp_odds.rank(method='min').astype(int)
            
            # odds/tansho_oddsã‚‚ä¸Šæ›¸ã
            df['odds'] = df['odds_snapshot']
            df['tansho_odds'] = df['odds_snapshot']
        else:
            logger.warning("Snapshot odds not available. Using parquet features intact (Potential Leak if past race).")
        
        # ãƒ¢ãƒ‡ãƒ«æ¨è«–
        feature_cols = self.models[0].feature_name()
        
        # ç‰¹å¾´é‡æº–å‚™
        for c in feature_cols:
            if c not in df.columns:
                df[c] = 0
        
        X = df[feature_cols].fillna(0)
        
        # Ensemble prediction
        preds = []
        for model in self.models:
            preds.append(model.predict(X))
        avg_pred = np.mean(preds, axis=0)
        
        # === paper_trade_run.py ã¨åŒã˜å¤‰æ› ===
        # Store raw logit
        df['raw_score'] = avg_pred
        
        # expit (sigmoid)
        df['prob_residual_raw'] = expit(avg_pred)
        
        # Calculate market probability from T-10m snapshot odds
        df['p_market_raw'] = 1.0 / df['odds_snapshot'].replace(0, np.nan)
        df['p_market'] = df['p_market_raw'] / df['p_market_raw'].sum()
        
        # Softmax per race
        def softmax_race(group):
            exp_vals = np.exp(group - group.max())
            return exp_vals / exp_vals.sum()
        
        df['prob_residual_softmax'] = softmax_race(df['prob_residual_raw'].values)
        
        # Calculate edge (model vs market) - for display
        df['edge'] = df['prob_residual_softmax'] - df['p_market']
        
        # score_logit_snap ã¯ raw_score ã«çµ±ä¸€ (è¡¨ç¤ºç”¨)
        df['score_logit_snap'] = df['raw_score']
        
        # ãƒ©ãƒ³ã‚¯ (softmaxç¢ºç‡ã§ãƒ©ãƒ³ã‚¯ä»˜ã‘)
        df['rank'] = df['prob_residual_softmax'].rank(ascending=False)
        
        return df.sort_values('rank')


class SanrenpukuBoxStrategy:
    """ä¸‰é€£è¤‡BOX4æˆ¦ç•¥"""
    
    def __init__(self, box_size: int = 4, bet_unit: int = 100):
        self.box_size = box_size
        self.bet_unit = bet_unit
    
    def generate_tickets(self, df: pd.DataFrame) -> List[Dict]:
        """è²·ã„ç›®ã‚’ç”Ÿæˆ"""
        top_horses = df.nsmallest(self.box_size, 'rank')['horse_number'].astype(int).tolist()
        
        tickets = []
        for combo in combinations(top_horses, 3):
            tickets.append({
                'type': 'sanrenpuku',
                'horses': sorted(combo),
                'bet': self.bet_unit
            })
        
        return tickets
    
    def format_tickets(self, tickets: List[Dict]) -> str:
        """è²·ã„ç›®ã‚’æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not tickets:
            return "è²·ã„ç›®ãªã—"
        
        horses = set()
        for t in tickets:
            horses.update(t['horses'])
        
        horses_str = '-'.join([f'{h:02}' for h in sorted(horses)])
        total_bet = sum(t['bet'] for t in tickets)
        n_tickets = len(tickets)
        
        return f"ğŸ¯ ä¸‰é€£è¤‡ BOX{self.box_size}\n`{horses_str}`\n{n_tickets}ç‚¹ Ã— Â¥{self.bet_unit} = Â¥{total_bet:,}"


class DiscordNotifier:
    """Discordé€šçŸ¥"""
    
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url
    
    def send(self, race_meta: Dict, df: pd.DataFrame, tickets_msg: str) -> bool:
        """äºˆæ¸¬çµæœã‚’Discordã«é€ä¿¡"""
        if not self.webhook_url:
            logger.error("Webhook URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        venue = VENUE_MAP.get(race_meta.get('venue', ''), race_meta.get('venue', ''))
        race_num = race_meta.get('race_number', '')
        title = race_meta.get('title', '')
        start_time = race_meta.get('start_time', '')
        
        header = f"ğŸ‡ ã€{venue}{race_num}Rã€‘{title} ({start_time})"
        
        # Top 5 äºˆæ¸¬
        top5 = df.nsmallest(5, 'rank')
        prediction_lines = []
        for _, row in top5.iterrows():
            h_num = f"{int(row['horse_number']):02}"
            h_name = row.get('horse_name', '')[:8]
            odds = row.get('odds_snapshot', row.get('odds', 0))
            score = row.get('score_logit_snap', 0)
            prediction_lines.append(f"`{h_num}` {h_name} (å˜{odds:.1f}, Sc:{score:.2f})")
        
        prediction_text = "\n".join(prediction_lines)
        
        embed = {
            "title": header,
            "description": f"**ğŸ“Š äºˆæ¸¬ (T-10m Odds)**\n{prediction_text}\n\n{tickets_msg}",
            "color": 0x00AA00,
            "footer": {"text": "Keiiba-AI v13 (Market Residual + T-10m Snapshot)"}
        }
        
        payload = {"username": "ç«¶é¦¬AI v13", "embeds": [embed]}
        
        try:
            resp = requests.post(self.webhook_url, json=payload, timeout=10)
            resp.raise_for_status()
            logger.info(f"é€šçŸ¥é€ä¿¡æˆåŠŸ: {race_meta.get('race_id', '')}")
            return True
        except Exception as e:
            logger.error(f"é€šçŸ¥é€ä¿¡å¤±æ•—: {e}")
            return False


class AutoPredictV13:
    """ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, dry_run: bool = False, target_date: str = None):
        self.dry_run = dry_run
        self.target_date = target_date
        self.state_file = STATE_FILE_PATH
        self.notified_races = self._load_state()
        
        load_env_manual()
        
        self.predictor = V13Predictor()
        self.strategy = SanrenpukuBoxStrategy(box_size=4)
        
        webhook_url = os.environ.get('DISCORD_WEBHOOK_URL')
        self.notifier = DiscordNotifier(webhook_url)
        
        self.engine = get_db_engine()
    
    def _load_state(self) -> set:
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r') as f:
                    return set(json.load(f))
            except:
                pass
        return set()
    
    def _save_state(self):
        if self.dry_run:
            return
        os.makedirs(os.path.dirname(self.state_file), exist_ok=True)
        with open(self.state_file, 'w') as f:
            json.dump(list(self.notified_races), f)
    
    def _load_race_list(self, date_str: str) -> pd.DataFrame:
        """å½“æ—¥ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’DBã‹ã‚‰å–å¾—"""
        # date_str: YYYYMMDD
        year = date_str[:4]
        mmdd = date_str[4:8]
        
        query = f"""
        SELECT 
            CONCAT(kaisai_nen, keibajo_code, kaisai_kai, kaisai_nichime, race_bango) as race_id,
            keibajo_code as venue,
            race_bango as race_number,
            kaisai_tsukihi,
            hasso_jikoku as start_time,
            kyosomei_hondai as title
        FROM jvd_ra
        WHERE kaisai_nen = '{year}'
          AND kaisai_tsukihi = '{mmdd}'
          AND keibajo_code BETWEEN '01' AND '10'
          AND data_kubun = '7'
        ORDER BY keibajo_code, race_bango
        """
        
        return pd.read_sql(query, self.engine)
    
    def _load_race_entries(self, race_id: str) -> pd.DataFrame:
        """å‡ºé¦¬è¡¨ã‚’å–å¾—"""
        kaisai_nen = race_id[:4]
        keibajo = race_id[4:6]
        kaisai_kai = race_id[6:8]
        kaisai_nichi = race_id[8:10]
        race_bango = race_id[10:12]
        
        query = f"""
        SELECT 
            res.umaban as horse_number,
            res.bamei as horse_name,
            res.kishu_code as jockey_id,
            res.tansho_odds as odds,
            res.wakuban as frame_number,
            res.futan_juryo as impost
        FROM jvd_se res
        WHERE res.kaisai_nen = '{kaisai_nen}'
          AND res.keibajo_code = '{keibajo}'
          AND res.kaisai_kai = '{kaisai_kai}'
          AND res.kaisai_nichime = '{kaisai_nichi}'
          AND res.race_bango = '{race_bango}'
        ORDER BY res.umaban
        """
        
        df = pd.read_sql(query, self.engine)
        df['horse_number'] = pd.to_numeric(df['horse_number'], errors='coerce')
        df['odds'] = pd.to_numeric(df['odds'], errors='coerce') / 10.0
        df['frame_number'] = pd.to_numeric(df['frame_number'], errors='coerce').fillna(0)
        df['impost'] = pd.to_numeric(df['impost'], errors='coerce').fillna(0)
        df['race_id'] = race_id
        
        return df
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç† (å˜æ—¥)"""
        logger.info("=== Auto Predict v13 é–‹å§‹ ===")
        
        now = datetime.now()
        if self.target_date:
            date_str = self.target_date.replace('-', '')
        else:
            date_str = now.strftime('%Y%m%d')
        
        logger.info(f"å¯¾è±¡æ—¥: {date_str}")
        
        race_list = self._load_race_list(date_str)
        if race_list.empty:
            logger.info("æœ¬æ—¥ã®ãƒ¬ãƒ¼ã‚¹ãªã—")
            return
            
        self._process_races(race_list, now if not self.target_date else None, date_str)
        
        self._save_state()
        logger.info("=== Auto Predict v13 å®Œäº† ===")

    def batch_run_year(self, year: str, jra_only: bool = True) -> pd.DataFrame:
        """æŒ‡å®šå¹´ã®å…¨ãƒ¬ãƒ¼ã‚¹ã‚’ã¾ã¨ã‚ã¦äºˆæ¸¬"""
        logger.info(f"=== Batch Predict Year: {year} ===")
        
        query = f"""
        SELECT DISTINCT kaisai_tsukihi as date
        FROM jvd_ra
        WHERE kaisai_nen = '{year}'
          AND data_kubun = '7'
        """
        if jra_only:
             query += " AND keibajo_code BETWEEN '01' AND '10'"
             
        df_dates = pd.read_sql(query, self.engine)
        dates = sorted(df_dates['date'].unique())
        logger.info(f"Found {len(dates)} dates.")
        
        all_results = []
        
        for d in dates:
            date_str = f"{year}{d}"
            # logger.info(f"Processing {date_str}...") # Reduce log noise
            
            race_list = self._load_race_list(date_str)
            if race_list.empty:
                continue
                
            results = self._process_races_batch(race_list, date_str)
            if results:
                all_results.extend(results)
                
        if not all_results:
            return pd.DataFrame()
            
        logger.info(f"Generated {len(all_results)} race predictions.")
        return pd.concat(all_results, ignore_index=True)

    def _process_races_batch(self, race_list: pd.DataFrame, date_str: str) -> List[pd.DataFrame]:
        """ãƒãƒƒãƒå‡¦ç†ç”¨"""
        targets = []
        for _, row in race_list.iterrows():
            race_id = row['race_id']
            start_time_str = str(row['start_time']).zfill(4)
            try:
                race_dt = datetime.strptime(f"{date_str}{start_time_str}", "%Y%m%d%H%M")
                targets.append((row, race_dt))
            except:
                continue
        
        if not targets:
            return []
            
        target_race_ids = [row['race_id'] for row, _ in targets]
        all_features_df = self.predictor.get_features(date_str, race_ids=target_race_ids)
        
        if all_features_df.empty:
            return []
            
        results = []
        for row, race_dt in targets:
            race_id = row['race_id']
            entries = all_features_df[all_features_df['race_id'].astype(str) == str(race_id)].copy()
            if entries.empty:
                continue
            
            # Strict Snapshot Odds (No Fallback)
            snapshot_odds = self.predictor.odds_fetcher.fetch_latest_odds(race_id, race_dt, before_minutes=10)
            
            if not snapshot_odds:
                continue
            
            # Cleanup parquet potential leaks before prediction
            # odds/popularity are overwritten by predict_race (safe), but rank/time must be dropped
            drop_cols = ['rank', 'time', 'agari', 'kakutei_chakujun']
            entries = entries.drop(columns=[c for c in drop_cols if c in entries.columns], errors='ignore')

            try:
                pred_df = self.predictor.predict_race(entries, snapshot_odds)
                
                # Metadata
                pred_df['race_id'] = race_id
                pred_df['post_time'] = race_dt
                pred_df['snapshot_time_used'] = race_dt - timedelta(minutes=10)
                pred_df['odds_tminus10m'] = pred_df['odds_snapshot']
                pred_df['popularity_tminus10m'] = pred_df['popularity']
                pred_df['p_market_tminus10m'] = pred_df['p_market']
                
                save_cols = [
                    'race_id', 'horse_number', 'post_time', 'snapshot_time_used',
                    'odds_tminus10m', 'popularity_tminus10m', 'p_market_tminus10m',
                    'raw_score', 'prob_residual_softmax', 'rank'
                ]
                if 'delta_logit' in pred_df.columns:
                     save_cols.append('delta_logit')
                     
                results.append(pred_df[save_cols])
            except Exception as e:
                # logger.error(f"Error predicting {race_id}: {e}")
                pass
                
        return results

    def _process_races(self, race_list: pd.DataFrame, now: Optional[datetime], date_str: str):
        """é€šå¸¸å®Ÿè¡Œç”¨"""
        targets = []
        for _, row in race_list.iterrows():
            race_id = row['race_id']
            if race_id in self.notified_races: continue
            
            start_time_str = str(row['start_time']).zfill(4)
            try:
                race_dt = datetime.strptime(f"{date_str}{start_time_str}", "%Y%m%d%H%M")
            except: continue
            
            if now:
                diff_min = (race_dt - now).total_seconds() / 60
                if 5 <= diff_min <= 15:
                    targets.append((row, race_dt))
            else:
                targets.append((row, race_dt))
        
        if not targets:
            logger.info("å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ãªã— (5-15åˆ†å‰ã®ãƒ¬ãƒ¼ã‚¹ãŒãªã„)")
            return

        target_race_ids = [r['race_id'] for r, _ in targets]
        all_features_df = self.predictor.get_features(date_str, race_ids=target_race_ids) # Parquet
        
        if all_features_df.empty: return

        for row, race_dt in targets:
            race_id = row['race_id']
            entries = all_features_df[all_features_df['race_id'].astype(str) == str(race_id)].copy()
            if entries.empty: continue
            
            # Drop leaks
            drop_cols = ['rank', 'time', 'agari', 'kakutei_chakujun']
            entries = entries.drop(columns=[c for c in drop_cols if c in entries.columns], errors='ignore')
            
            snapshot_odds = self.predictor.odds_fetcher.fetch_latest_odds(race_id, race_dt, before_minutes=10)
            if not snapshot_odds:
                logger.warning(f"ã‚ªãƒƒã‚ºå–å¾—å¤±æ•—: {race_id}")
                continue # No Fallback

            try:
                result_df = self.predictor.predict_race(entries, snapshot_odds)
                tickets = self.strategy.generate_tickets(result_df)
                tickets_msg = self.strategy.format_tickets(tickets)
                
                race_meta = {
                    'race_id': race_id, 'venue': row['venue'], 'race_number': int(row['race_number']),
                    'title': row['title'] or '', 'start_time': f"{start_time_str[:2]}:{start_time_str[2:]}"
                }
                
                if self.dry_run:
                    logger.info(f"[DRY-RUN] {race_meta}")
                    print(result_df[['horse_number', 'horse_name', 'odds_snapshot', 'popularity', 'score_logit_snap', 'rank']].head(8))
                    print(tickets_msg)
                else:
                    if self.notifier.send(race_meta, result_df, tickets_msg):
                        self.notified_races.add(race_id)
                    time.sleep(1.5)
            except Exception as e:
                logger.error(f"Error {race_id}: {e}")


def main():
    parser = argparse.ArgumentParser(description='Auto Predict v13 (T-10m Odds + ä¸‰é€£è¤‡BOX4)')
    parser.add_argument('--dry-run', action='store_true', help='é€šçŸ¥ã‚’é€ä¿¡ã›ãšã«å®Ÿè¡Œ')
    parser.add_argument('--date', type=str, help='å¯¾è±¡æ—¥ä»˜ (YYYY-MM-DD or YYYYMMDD)')
    parser.add_argument('--year', type=str, help='æŒ‡å®šå¹´ã‚’ã¾ã¨ã‚ã¦å‡¦ç† (Batch Mode)')
    parser.add_argument('--jra_only', action='store_true', default=True, help='JRAã®ã¿')
    parser.add_argument('--out', type=str, help='å‡ºåŠ›å…ˆparquetãƒ‘ã‚¹ (Batch Modeç”¨)')
    parser.add_argument('--run_leak_proof', action='store_true', help='Leak Proof Mode Flag (Dummy for compatibility)')
    
    args = parser.parse_args()
    
    predictor = AutoPredictV13(dry_run=args.dry_run, target_date=args.date)
    
    if args.year:
        if not args.out:
            print("Error: --out is required when using --year")
            return
        df = predictor.batch_run_year(args.year, args.jra_only)
        if not df.empty:
            os.makedirs(os.path.dirname(os.path.abspath(args.out)), exist_ok=True)
            df.to_parquet(args.out)
            logger.info(f"Saved {len(df)} rows to {args.out}")
        else:
            logger.warning("No predictions generated.")
    else:
        predictor.run()


if __name__ == "__main__":
    main()

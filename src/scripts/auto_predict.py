import os
import sys
import json
import time
import requests
import argparse
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from scipy.special import softmax

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.model.ensemble import EnsembleModel
from src.inference.preprocessor import InferencePreprocessor
from src.inference.loader import InferenceDataLoader
from src.model.calibration import ProbabilityCalibrator

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(os.path.join(os.path.dirname(__file__), '../../logs/auto_predict.log'))
    ]
)
logger = logging.getLogger(__name__)


# .env æ‰‹å‹•èª­ã¿è¾¼ã¿ (Dockerç’°å¢ƒç­‰ã§ç’°å¢ƒå¤‰æ•°ãŒåæ˜ ã•ã‚Œã¦ã„ãªã„å ´åˆç”¨)
def load_env_manual():
    try:
        env_path = os.path.join(os.path.dirname(__file__), '../../.env')
        if os.path.exists(env_path):
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'): continue
                    if '=' in line:
                        key, val = line.split('=', 1)
                        os.environ[key.strip()] = val.strip()
            # logger.info(".env loaded manually.")
    except Exception as e:
        logger.warning(f".env reading failed: {e}")

# å®šæ•°
STATE_FILE_PATH = os.path.join(os.path.dirname(__file__), '../../data/state/notified_races.json')
# DISCORD_WEBHOOK_URL will be loaded dynamically

class NotificationManager:
    """Discordé€šçŸ¥ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url

    def send_prediction(self, race_meta: Dict, df: pd.DataFrame):
        """
        äºˆæ¸¬çµæœã‚’Discordã«é€ä¿¡ã—ã¾ã™ã€‚
        
        Args:
            race_meta: ãƒ¬ãƒ¼ã‚¹æƒ…å ± (race_id, title, venue, etc.)
            df: äºˆæ¸¬çµæœDataFrame (é¦¬ç•ª,é¦¬å,EV,ç¢ºç‡ãªã©ã‚’å«ã‚€)
        """
        if not self.webhook_url:
            logger.warning("Discord Webhook URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚é€šçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        # ã‚¿ã‚¤ãƒˆãƒ«æ•´å½¢
        date_str = race_meta.get('date', '')
        title_str = f"ğŸ¯ [{date_str}] {race_meta['venue_name']}{race_meta['race_number']}R {race_meta['title']} ({race_meta['start_time']})"

        # äºˆæ¸¬ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ (Top 6: å‹ç‡é †)
        top_picks = df.sort_values('calibrated_prob', ascending=False).head(6)
        
        description = "**ğŸ† æœ¬å‘½äºˆæ¸¬ (å‹ç‡ä¸Šä½)**\n"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã€ãƒªã‚¹ãƒˆå½¢å¼ã§è¦‹ã‚„ã™ã
        marks = ["â—", "ã€‡", "â–²", "â–³", "â–³", "â–³"]
        
        for i, (_, row) in enumerate(top_picks.iterrows()):
            mark = marks[i] if i < len(marks) else ""
            h_num = str(int(row['horse_number'])).zfill(2)
            h_name = row['horse_name']
            
            ev = f"{row['expected_value']:.2f}"
            prob = f"{row['calibrated_prob']*100:.0f}%"
            score = f"{row['score']:.2f}"
            
            # Simple list format with Score
            description += f"`{mark}` `{h_num}` **{h_name}** (å‹ç‡:{prob}, EV:{ev}, Sc:{score})\n"

        # æ¨å¥¨è²·ã„ç›® (Smart Value Logic)
        bet_strategy = self._generate_betting_strategy(df)
        
        embed = {
            "title": title_str,
            "description": description + "\n" + bet_strategy,
            "color": 0xFF0000 if top_picks.iloc[0]['expected_value'] > 1.5 else 0x00FF00, # é«˜æœŸå¾…å€¤ãªã‚‰èµ¤
            "footer": {
                "text": "Keiiba-AI Prediction System"
            }
        }
        
        payload = {
            "username": "ãƒŠãƒŸãƒ¼ãƒ«",
            "embeds": [embed]
        }
        
        try:
            resp = requests.post(self.webhook_url, json=payload)
            resp.raise_for_status()
            logger.info(f"é€šçŸ¥é€ä¿¡æˆåŠŸ: {race_meta['race_id']}")
            return True
        except Exception as e:
            logger.error(f"é€šçŸ¥é€ä¿¡å¤±æ•—: {e}")
            return False

    def _pad_width(self, s: str, width: int) -> str:
        """å…¨è§’æ–‡å­—ã‚’è€ƒæ…®ã—ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ç°¡æ˜“é–¢æ•°"""
        # å³å¯†ãªè¨ˆç®—ã¯è¤‡é›‘ãªã®ã§ã€å…¨è§’=2ã€åŠè§’=1ã¨ã—ã¦è¨ˆç®—ã—ã¦ã‚¹ãƒšãƒ¼ã‚¹ã§åŸ‹ã‚ã‚‹
        count = 0
        for c in s:
            if ord(c) > 255: count += 2
            else: count += 1
        
        padding = width - count
        if padding > 0:
            return s + " " * padding
        else:
            return s

    def _generate_betting_strategy(self, df: pd.DataFrame) -> str:
        """æ¨å¥¨è²·ã„ç›®ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ (Smart Value Logic)"""
        # 1. å‹ç‡ä¸Šä½6é ­ã‚’æŠ½å‡º (å®‰å®šç¾¤)
        top_prob_df = df.sort_values('calibrated_prob', ascending=False).head(6)
        
        # 2. ãã®ä¸­ã§æœ€ã‚‚EVãŒé«˜ã„é¦¬ã‚’ã€Œç‹™ã„ç›®ã€ã¨ã™ã‚‹
        best_smart_horse = top_prob_df.sort_values('expected_value', ascending=False).iloc[0]
        
        # 3. ç´”ç²‹ãªå‹ç‡1ä½ (æœ¬å‘½)
        best_prob_horse = top_prob_df.iloc[0]
        
        msg = "**ğŸ« æ¨å¥¨è²·ã„ç›®**\n"
        
        # A. æœ¬å‘½ (å‹ç‡ 1ä½)
        p_num = int(best_prob_horse['horse_number'])
        p_name = best_prob_horse['horse_name']
        p_prob = best_prob_horse['calibrated_prob']
        p_ev = best_prob_horse['expected_value']
        
        msg += f"ğŸ‘‘ **æœ¬å‘½ (å …å®Ÿ)**: {p_num} {p_name}\n"
        msg += f"   (å‹ç‡: {p_prob*100:.1f}%, EV: {p_ev:.2f}) -> å˜å‹/é€£è»¸\n"
        
        # B. ç‹™ã„ç›® (ä¸Šä½5é ­ã®ä¸­ã§Best EV)
        # æœ¬å‘½ã¨ç•°ãªã‚‹å ´åˆã®ã¿è¡¨ç¤º
        if int(best_smart_horse['horse_number']) != p_num:
            v_num = int(best_smart_horse['horse_number'])
            v_name = best_smart_horse['horse_name']
            v_prob = best_smart_horse['calibrated_prob']
            v_ev = best_smart_horse['expected_value']
            
            # EVãŒ1.0ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã®ã¿æ¨å¥¨
            if v_ev > 1.0:
                msg += f"ğŸ’° **ç‹™ã„ç›® (é«˜æœŸå¾…å€¤)**: {v_num} {v_name}\n"
                msg += f"   (å‹ç‡: {v_prob*100:.1f}%, EV: {v_ev:.2f}) -> å˜è¤‡/ãƒ¯ã‚¤ãƒ‰ç›¸æ‰‹\n"
        
        # å…¨ä½“çš„ãªã‚³ãƒ¡ãƒ³ãƒˆ
        if p_ev < 1.0 and best_smart_horse['expected_value'] < 1.0:
            msg += "\nâš ï¸ **å…¨ä½“çš„ã«æœŸå¾…å€¤ä½ã‚ (è¦‹é€ã‚Šæ¨å¥¨)**\n"
            
        return msg

class AutoPredictor:
    def __init__(self, dry_run: bool = False, target_date: str = None):
        self.dry_run = dry_run
        self.target_date = target_date
        self.state_file = STATE_FILE_PATH
        self.notified_races = self._load_state()
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– (åˆå›ã®ã¿ãƒ­ãƒ¼ãƒ‰)
        self.loader = InferenceDataLoader()
        self.preprocessor = InferencePreprocessor()
        self.calibrator = self._load_calibrator()
        self.model = self._load_model() # Ensemble
        
        # Load env vars manually to ensure Webhook URL is present
        load_env_manual()
        webhook_url = os.environ.get('DISCORD_WEBHOOK_URL')
        if not webhook_url:
            logger.error("âŒ DISCORD_WEBHOOK_URL is not set. Notifications will fail.")
            
        self.notifier = NotificationManager(webhook_url)
        
    def _load_state(self) -> set:
        """é€šçŸ¥æ¸ˆã¿ãƒ¬ãƒ¼ã‚¹IDã®èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r') as f:
                    return set(json.load(f))
            except:
                return set()
        return set()

    def _save_state(self):
        """é€šçŸ¥æ¸ˆã¿ãƒ¬ãƒ¼ã‚¹IDã®ä¿å­˜"""
        if self.dry_run: return
        
        os.makedirs(os.path.dirname(self.state_file), exist_ok=True)
        with open(self.state_file, 'w') as f:
            json.dump(list(self.notified_races), f)

    def _load_model(self):
        logger.info("ãƒ¢ãƒ‡ãƒ«(Ensemble)ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        model = EnsembleModel()
        model_dir = os.path.join(os.path.dirname(__file__), '../../models')
        # å„ªå…ˆé †ä½: v4_2025 ã®ã¿ (v1ç­‰ã¯ç‰¹å¾´é‡ä¸æ•´åˆã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŸã‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ãªã„)
        path = os.path.join(model_dir, 'ensemble_v4_2025.pkl')
        # if not os.path.exists(path): path = os.path.join(model_dir, 'ensemble_v1.pkl') # å‰Šé™¤
        # if not os.path.exists(path): path = os.path.join(model_dir, 'ensemble_model.pkl') # å‰Šé™¤
        
        if os.path.exists(path):
            model.load_model(path)
            return model
        else:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
            return None

    def _load_calibrator(self):
        model_dir = os.path.join(os.path.dirname(__file__), '../../models')
        path = os.path.join(model_dir, 'calibrator.pkl')
        if os.path.exists(path):
            try:
                from src.model.calibration import ProbabilityCalibrator
                calib = ProbabilityCalibrator()
                calib.load(path)
                return calib
            except:
                return None
        return None

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ«ãƒ¼ãƒ—"""
        logger.info("è‡ªå‹•äºˆæ¸¬ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™...")
        
        # 1. é–‹å‚¬æ—¥/ç¾åœ¨æ™‚åˆ»ã®å–å¾—
        now = datetime.now()
        if self.target_date:
            today_str = self.target_date.replace('-', '')
            # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã¯æ™‚åˆ»ã‚’ä»»æ„ã«è¨­å®šã§ããªã„ãŸã‚ã€å…¨ãƒ¬ãƒ¼ã‚¹å¯¾è±¡ã«ã™ã‚‹ç­‰ã®å·¥å¤«ãŒå¿…è¦ã ãŒã€
            # ã“ã“ã§ã¯ã€Œãã®æ—¥ã®ãƒ¬ãƒ¼ã‚¹å…¨ã¦ã€ã‚’ã€Œæœªé€šçŸ¥ãªã‚‰ã€å‡¦ç†ã™ã‚‹å‹•ãã«ãªã‚‹ã€‚
            # ãŸã ã—ç›´å‰ãƒã‚§ãƒƒã‚¯ã‚‚ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹å ´åˆã¯æ™‚åˆ»ãƒ¢ãƒƒã‚¯ãŒå¿…è¦ã€‚
            # ä»Šå›ã¯ç°¡æ˜“çš„ã«ã€ŒæŒ‡å®šæ—¥ãªã‚‰å…¨ãƒ¬ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯ã€ã¨ã™ã‚‹ã€‚
        else:
            today_str = now.strftime('%Y%m%d')

        # 2. ãƒ¬ãƒ¼ã‚¹ä¸€è¦§å–å¾—
        race_list_df = self.loader.load_race_list(today_str)
        if race_list_df.empty:
            logger.info("æœ¬æ—¥ã®é–‹å‚¬ãƒ¬ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return

        # 3. ç›´å‰ãƒ¬ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (ç™ºèµ° 15åˆ†ã€œ30åˆ†å‰)
        targets = []
        
        for _, row in race_list_df.iterrows():
            race_id = row['race_id']
            if race_id in self.notified_races:
                continue
                
            start_time_str = row['start_time'] # HHMM format usually "1000"
            if not start_time_str: continue

            # æ™‚åˆ»ãƒ‘ãƒ¼ã‚¹
            try:
                # today_str (YYYYMMDD) + start_time_str (HHMM)
                race_dt = datetime.strptime(f"{today_str}{start_time_str}", "%Y%m%d%H%M")
            except ValueError:
                continue

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¤å®š
            if self.target_date:
                # æŒ‡å®šæ—¥ãƒ¢ãƒ¼ãƒ‰ãªã‚‰ç„¡æ¡ä»¶ã«è¿½åŠ  (ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ç”¨)
                targets.append(row)
            else:
                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ¼ãƒ‰
                diff = race_dt - now
                minutes = diff.total_seconds() / 60
                
                # 15åˆ†ã€œ35åˆ†å‰ãã‚‰ã„ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹
                if 15 <= minutes <= 35:
                     targets.append(row)
        
        if not targets:
            logger.info("ç¾åœ¨ã€ç›´å‰ã®é€šçŸ¥å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
            
        logger.info(f"é€šçŸ¥å¯¾è±¡ãƒ¬ãƒ¼ã‚¹: {len(targets)} ä»¶")

        # 4. æ¨è«– & é€šçŸ¥
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã€å¯¾è±¡ãƒ¬ãƒ¼ã‚¹åˆ†ã¾ã¨ã‚ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ãƒ«ãƒ¼ãƒ—ã™ã‚‹ã‹ã€‚
        # InferenceDataLoader.load ã¯ race_ids ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Œã‚‹ãŒã€
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰¹å¾´é‡ã®ãŸã‚ã«ã€Œãã®æ—¥ã®å…¨çµæœã€ã‚‚å¿…è¦ã€‚
        # Loaderã®ä»•æ§˜ä¸Šã€race_idsã‚’æŒ‡å®šã—ã¦ã‚‚å†…éƒ¨ã§æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ã®ã¿ã«ã—ã¦å…¨ä»¶ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚ˆã†å®Ÿè£…ä¿®æ­£æ¸ˆã¿ãªã‚‰OKã€‚
        # ç¢ºèª: loader.py:228 ã§ã€Œå‘¼ã³å‡ºã—å…ƒã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€ã¨ãªã£ã¦ã„ã‚‹ã€‚
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ (æ—¥æ¬¡ã§ä¸€æ‹¬ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ¡ãƒ¢ãƒªã«ä¹—ã›ã¦ãŠãã®ãŒç†æƒ³ã ãŒã€ã“ã“ã§ã¯æ¯å›ãƒ­ãƒ¼ãƒ‰)
        target_ids = [r['race_id'] for r in targets]
        
        # Loaderã¯ã€ŒæŒ‡å®šæ—¥ä»˜ã®å…¨ãƒ¬ãƒ¼ã‚¹ã€ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€race_idsã§ãƒ•ã‚£ãƒ«ã‚¿ã—ã¦ã„ãªã„ï¼ˆLoaderä¿®æ­£æ¬¡ç¬¬ï¼‰ã€‚
        # ç¾çŠ¶ã®Loaderã¯ race_ids ã‚’æ¸¡ã™ã¨ SQL ã® WHERE IN ã«å…¥ã‚Œã‚‹ãŒã€
        # RealTimeFeatureã®ãŸã‚ã«ã€ŒåŒæ—¥ã®çµ‚äº†ã—ãŸãƒ¬ãƒ¼ã‚¹ã€ãŒå¿…è¦ãªå ´åˆã€ã“ã‚Œã§ã¯ä¸è¶³ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
        # -> Loaderä¿®æ­£æ¸ˆã¿: race_idsã‹ã‚‰æ—¥ä»˜ã‚’å–ã‚Šå‡ºã—ã¦ãã®æ—¥ã®å…¨ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—ã™ã‚‹ã‚ˆã†ã«ã—ãŸã‹ï¼Ÿ
        # -> ã¯ã„ã€loader.py ã® 226è¡Œç›®ä»˜è¿‘ã§å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚
        
        try:
            raw_df = self.loader.load(target_date=today_str, race_ids=target_ids)
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return

        if raw_df.empty:
            return

        # å‰å‡¦ç†
        try:
            X, ids = self.preprocessor.preprocess(raw_df)
            processed_df = pd.concat([ids, X], axis=1)
            # é‡è¤‡ã‚«ãƒ©ãƒ ã‚’å‰Šé™¤ (idsã¨Xã§é‡è¤‡ãŒã‚ã‚‹å ´åˆã€idså´=å·¦å´ã‚’å„ªå…ˆã—ã¦æ®‹ã™)
            processed_df = processed_df.loc[:, ~processed_df.columns.duplicated()]
        except Exception as e:
            logger.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return
            
        # äºˆæ¸¬
        # feature columns alignment
        feature_cols = None
        # ... predict.pyã¨åŒæ§˜ã®ç‰¹å¾´é‡åè§£æ±ºãƒ­ã‚¸ãƒƒã‚¯ ...
        # ç°¡æ˜“åŒ–ã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ãŒ feature_name() ã‚’æŒã£ã¦ã„ã‚‹ã¨ä»®å®š
        # EnsembleModelã®å ´åˆã€å†…éƒ¨ã®ãƒ¢ãƒ‡ãƒ«(LGBM)ã‹ã‚‰ç‰¹å¾´é‡åã‚’å–å¾—ã™ã‚‹
        if isinstance(self.model, EnsembleModel):
            bst = self.model.lgbm.model
        else:
            bst = self.model.model

        logger.info(f"DEBUG: bst type: {type(bst)}")
        logger.info(f"DEBUG: dir(bst): {dir(bst)[:20]}...") # show first 20 attrs


        if hasattr(bst, 'feature_name'):
             feature_cols = bst.feature_name()
             logger.info(f"DEBUG: bst.feature_name() found. Len: {len(feature_cols)}")
        elif hasattr(bst, 'booster_'):
             feature_cols = bst.booster_.feature_name()
             logger.info(f"DEBUG: bst.booster_.feature_name() found. Len: {len(feature_cols)}")
        
        if feature_cols:
             logger.info(f"DEBUG: Using {len(feature_cols)} features for prediction.")
             # Add missing as 0
             missing = set(feature_cols) - set(processed_df.columns)
             if missing:
                 logger.info(f"DEBUG: Missing columns: {missing}")
                 for c in missing: processed_df[c] = 0
             
             X_pred = processed_df[feature_cols]
        else:
             logger.warning("DEBUG: Feature names NOT found in model. Using all numeric columns.")
             X_pred = processed_df.select_dtypes(include=[np.number])
        
        # Check for duplicates
        if X_pred.columns.duplicated().any():
            logger.warning(f"DEBUG: X_pred has duplicated columns: {X_pred.columns[X_pred.columns.duplicated()].tolist()}")
            X_pred = X_pred.loc[:, ~X_pred.columns.duplicated()]
            
        logger.info(f"DEBUG: X_pred shape checks - Shape: {X_pred.shape}")

             
        try:
            scores = self.model.predict(X_pred)
            processed_df['score'] = scores
            
            # Softmax
            processed_df['prob'] = processed_df.groupby('race_id')['score'].transform(lambda x: softmax(x))
            
            # Calibrate
            if self.calibrator:
                processed_df['calibrated_prob'] = self.calibrator.predict(processed_df['prob'].values)
            else:
                processed_df['calibrated_prob'] = processed_df['prob']
                
            # EV
            if 'odds' in processed_df.columns:
                processed_df['expected_value'] = processed_df['calibrated_prob'] * processed_df['odds'].fillna(0)
            else:
                processed_df['expected_value'] = 0
                
        except Exception as e:
            logger.error(f"äºˆæ¸¬å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return

        # 5. å„ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«é€šçŸ¥
        for race_meta in targets:
            race_id = race_meta['race_id']
            
            # ã“ã®ãƒ¬ãƒ¼ã‚¹ã®é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            race_df = processed_df[processed_df['race_id'] == race_id].copy()
            if race_df.empty: continue
            
            venue_map = {
                '01': 'æœ­å¹Œ', '02': 'å‡½é¤¨', '03': 'ç¦å³¶', '04': 'æ–°æ½Ÿ', '05': 'æ±äº¬', 
                '06': 'ä¸­å±±', '07': 'ä¸­äº¬', '08': 'äº¬éƒ½', '09': 'é˜ªç¥', '10': 'å°å€‰'
            }
            venue_code = race_meta['venue']
            race_meta_dict = {
                'race_id': race_id,
                'title': race_meta['title'],
                'race_number': race_meta['race_number'],
                'start_time': race_meta['start_time'][:2] + ":" + race_meta['start_time'][2:],
                'venue_name': venue_map.get(venue_code, 'Unknown'),
                'date': self.target_date if self.target_date else datetime.now().strftime('%Y/%m/%d')
            }
            
            logger.info(f"é€šçŸ¥é€ä¿¡: {race_meta_dict['title']}")
            
            if not self.dry_run:
                success = self.notifier.send_prediction(race_meta_dict, race_df)
                if success:
                    self.notified_races.add(race_id)
                time.sleep(1.0) # Rate Limitå›é¿ (1ç§’å¾…æ©Ÿ)
            else:
                logger.info("DRY-RUN: é€šçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
                print(race_df[['horse_name', 'expected_value']].sort_values('expected_value', ascending=False).head())

        # å®Œäº†å¾Œã«çŠ¶æ…‹ä¿å­˜
        self._save_state()


def main():
    parser = argparse.ArgumentParser(description='Automated Prediction & Notification')
    parser.add_argument('--dry-run', action='store_true', help='é€šçŸ¥ã‚’é€ä¿¡ã›ãšã«å®Ÿè¡Œ')
    parser.add_argument('--date', type=str, help='å¯¾è±¡æ—¥ä»˜ (YYYY-MM-DD)')
    args = parser.parse_args()
    
    predictor = AutoPredictor(dry_run=args.dry_run, target_date=args.date)
    predictor.run()

if __name__ == "__main__":
    main()

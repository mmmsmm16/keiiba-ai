import pandas as pd
import numpy as np
from typing import Dict, List
import logging

logger = logging.getLogger(__name__)

class DatasetSplitter:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å­¦ç¿’ç”¨ãƒ»æ¤œè¨¼ç”¨ãƒ»ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ã—ã€
    LightGBM (Ranking) ã§å­¦ç¿’å¯èƒ½ãªå½¢å¼ã«æ•´å½¢ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    
    @staticmethod
    def _create_ranking_target(rank: int) -> int:
        """v12äº’æ›: ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ (1ç€=3, 2ç€=2, 3ç€=1, ç€å¤–=0)"""
        if rank == 1:
            return 3
        elif rank == 2:
            return 2
        elif rank == 3:
            return 1
        else:
            return 0
    
    @staticmethod
    def _create_v13_graded_target(rank: int) -> float:
        """v13ç”¨: è¤‡å‹åœã‚°ãƒ¬ãƒ¼ãƒ‰ä»˜ãã‚¿ãƒ¼ã‚²ãƒƒãƒˆ (1ç€=1.0, 2ç€=0.5, 3ç€=0.25, ç€å¤–=0)"""
        if rank == 1:
            return 1.0
        elif rank == 2:
            return 0.5
        elif rank == 3:
            return 0.25
        else:
            return 0.0
    
    def split_and_create_dataset(self, df: pd.DataFrame, valid_year: int = 2025,
                                  target_type: str = "ranking") -> Dict[str, Dict]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å‰å‡¦ç†æ¸ˆã¿ã®å…¨ãƒ‡ãƒ¼ã‚¿ã€‚
            valid_year (int): æ¤œè¨¼ã«ä½¿ç”¨ã™ã‚‹å¹´ã€‚Trainã¯ã“ã‚Œã‚ˆã‚Šå‰ã®å¹´ã€Testã¯ã“ã‚Œã‚ˆã‚Šå¾Œã®å¹´ã«ãªã‚‹ã€‚
            target_type (str): ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç¨®åˆ¥ ("ranking" or "v13_graded")

        Returns:
            Dict: train, valid, test ãã‚Œãã‚Œã® {'X', 'y', 'group'} ã‚’å«ã‚€è¾æ›¸ã€‚
        """
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²ã¨ä½œæˆã‚’é–‹å§‹ (Valid Year: {valid_year}, Target Type: {target_type})...")

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ä½œæˆ
        if 'target' not in df.columns:
            if target_type == "v13_graded":
                # v13å …å®Ÿãƒ¢ãƒ‡ãƒ«ç”¨: è¤‡å‹åœã‚°ãƒ¬ãƒ¼ãƒ‰ä»˜ã
                logger.info("ğŸ“Š v13ç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆ: 1ç€=1.0, 2ç€=0.5, 3ç€=0.25, ç€å¤–=0")
                df['target'] = df['rank'].apply(self._create_v13_graded_target)
            else:
                # v12äº’æ›: ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ (1ç€=3, 2ç€=2, 3ç€=1, ç€å¤–=0)
                df['target'] = df['rank'].apply(self._create_ranking_target)

        # æ™‚ç³»åˆ—åˆ†å‰²
        # Train: 2010 ~ valid_year - 1 (Expanded start range)
        # Valid: valid_year
        # Test: valid_year + 1 ~
        train_df = df[df['year'] < valid_year].copy()
        valid_df = df[df['year'] == valid_year].copy()
        test_df = df[df['year'] > valid_year].copy()

        logger.info(f"åˆ†å‰²å®Œäº†: Train({len(train_df)}), Valid({len(valid_df)}), Test({len(test_df)})")

        return {
            'train': self._create_lgbm_dataset(train_df),
            'valid': self._create_lgbm_dataset(valid_df),
            'test': self._create_lgbm_dataset(test_df)
        }

    def _create_lgbm_dataset(self, df: pd.DataFrame) -> Dict:
        """
        DataFrameã‹ã‚‰LightGBMç”¨ã® X, y, group ã‚’ä½œæˆã—ã¾ã™ã€‚
        """
        if df.empty:
            return {'X': pd.DataFrame(), 'y': pd.Series(), 'group': np.array([])}

        # [Fix] LambdaRankç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚½ãƒ¼ãƒˆ (æ—¥ä»˜ -> ãƒ¬ãƒ¼ã‚¹ID)
        # ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³é †ã¨groupã®ä¸¦ã³é †ãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        df = df.sort_values(['date', 'race_id'])

        # ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ± (Queryå˜ä½ã®ãƒ‡ãƒ¼ã‚¿æ•°)
        # sort=False ã«ã™ã‚‹ã“ã¨ã§ã€dfã®ä¸¦ã³é †(date, race_idé †)ã‚’ç¶­æŒã—ãŸã¾ã¾ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹
        # ã€Importantã€‘LambdaRank/YetiRankã§ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚¯ã‚¨ãƒªã”ã¨ã«é€£ç¶šã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹
        # [Fix] LambdaRank requires data to be sorted by query (race_id) blocks.
        # df is sorted, but standard groupby().size() aggregates disjoint blocks if they share the same key.
        # To strictly match the physical dataframe order, we use Run-Length Encoding (RLE).
        
        # 1. Identify where race_id changes
        is_new_group = df['race_id'] != df['race_id'].shift()
        group_ids = is_new_group.cumsum()
        
        # 2. Count size of each physical block
        group = df.groupby(group_ids, sort=False).size().to_numpy()
        
        # [Validation] Disjoint Check
        # If n_groups > nunique, it means some race_ids are split (disjoint).
        n_unique_races = df['race_id'].nunique()
        if len(group) != n_unique_races:
             # Find which races are disjoint
             dup_counts = df.groupby('race_id').size()
             # This is just counts, not blocks.
             # We just warn for now.
             print(f"[WARNING] Disjoint race_ids detected! Groups: {len(group)}, Unique: {n_unique_races}")
             # This implies data quality issue, but RLE enables training to proceed safely.

        # [Validation] Group Consistency Check
        # 1. Sum of groups must equal total rows
        assert group.sum() == len(df), f"Group sum {group.sum()} != len(df) {len(df)}"
        # 2. Number of groups must match physical blocks (Guaranteed by logic above)
        # assert len(group) == n_unique_races <-- Disabled because we handle disjoint now
        
        # 3. [Strict] Verify contiguous blocks (Race ID Order Mismatch Check)
        # LambdaRank requires data to be sorted by query (race_id) blocks.
        # Check if df matches the group boundaries exactly.
        current_idx = 0
        for i, size in enumerate(group):
            # Check strictly if the chunk contains only one race_id
            # Using .iloc is fast enough for ~50k groups
            chunk_race_id = df['race_id'].iloc[current_idx]
            # Just checking the first row is basically enough if we assume sorted, 
            # but to be 100% sure we check nunique if performance allows. 
            # Doing .iloc[slice].nunique() for every group might be slow (40k calls).
            # Optimization: check first and last of the chunk. If sorted, they must match.
            chunk_race_id_last = df['race_id'].iloc[current_idx + size - 1]
            
            # [Fix] Do not compare with unique_races[i] because disjoint groups break the index alignment.
            # Only check self-consistency (start matches end).
            
            if chunk_race_id != chunk_race_id_last:
                 raise ValueError(f"Group {i} is not contiguous! Found {chunk_race_id} at start and {chunk_race_id_last} at end.")
            
            current_idx += size


        # ç‰¹å¾´é‡ (X) ã¨ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ (y) ã®åˆ†é›¢
        # ã€é‡è¦ã€‘æœªæ¥æƒ…å ±ï¼ˆãƒ¬ãƒ¼ã‚¹çµæœï¼‰ã‚’å«ã‚€ã‚«ãƒ©ãƒ ã¯å…¨ã¦å‰Šé™¤ã™ã‚‹
        drop_cols = [
            # IDãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (v09: ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ã†ãŸã‚ä¸€éƒ¨æ®‹ã™)
            'race_id', 'date', 'title', 'mare_id',
            'horse_name', # åå‰ã¯åŸºæœ¬ä¸è¦
            # ç›®çš„å¤‰æ•°
            'rank', 'target', 'rank_str',
            # æœªæ¥æƒ…å ± (Result)
            'time', 'raw_time',       # â† raw_time (1355ãªã©) ãŒæ®‹ã£ã¦ã„ã‚‹ã¨å³ãƒªãƒ¼ã‚¯
            'passing_rank',           # é€šéé †
            'last_3f',                # ä¸ŠãŒã‚Š3F
            'odds', 'popularity',     # ã‚ªãƒƒã‚ºãƒ»äººæ°—
            'weight',                 # å½“æ—¥é¦¬ä½“é‡
            # 'weight_diff',          # â† æœ‰åŠ¹åŒ– (Advanced Featuresã§ç”Ÿæˆ)
            'weight_diff_val', 'weight_diff_sign', # å…ƒãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚‹å ´åˆã¯å‰Šé™¤ï¼ˆé‡è¤‡å›é¿ï¼‰
            'winning_numbers', 'payout', 'ticket_type', # æ‰•ã„æˆ»ã—
            # PC-KEIBAç‰¹æœ‰ã®ã‚«ãƒ©ãƒ ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
            'pass_1', 'pass_2', 'pass_3', 'pass_4',
            
            # --- Leakage Features to Drop (Phase 11.1 fix) ---
            # These are derived from current race result or future odds
            'slow_start_recovery', 'pace_disadvantage', 'wide_run',
            'track_bias_disadvantage', 'outer_frame_disadv',
            'odds_race_rank', 'popularity_race_rank',
            'odds_deviation', 'popularity_deviation',
            
            # --- v11: trend_* (äº‹å‰äºˆæ¸¬ãƒ¢ãƒ¼ãƒ‰ç”¨ã®ãŸã‚drop) ---
            # realtime=OFFæ™‚ã¯ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«åŸ‹ã‚ã•ã‚Œã¦ã„ã‚‹ãŸã‚æƒ…å ±é‡ãªã—
            'trend_win_inner_rate', 'trend_win_mid_rate', 'trend_win_outer_rate',
            'trend_win_front_rate', 'trend_win_fav_rate',
            
            # --- Low Impact Features to Drop (v5 Feature Selection) ---
            # ä»Šå›(v8)ã¯å†è©•ä¾¡ã®ãŸã‚æ®‹ã™
            # 'race_avg_prize',         # é‡è¦åº¦ 0
            # 'race_pace_cat',          # é‡è¦åº¦ 0
            # 'total_prize',            # é‡è¦åº¦ 0
            # 'is_long_break',          # é‡è¦åº¦ 0
            # 'race_nige_horse_count',  # é‡è¦åº¦ 9
            # 'race_nige_bias',         # é‡è¦åº¦ 46
            # 'horse_pace_disadv_rate', # é‡è¦åº¦ 74
            # 'weather_num',            # é‡è¦åº¦ 92
            # 'weekday',                # é‡è¦åº¦ 119
            
            # --- v6 Ineffective Features (é‡è¦åº¦ 0) ---
            # 'frame_zone',             # é‡è¦åº¦ 0
            # 'distance_category',      # é‡è¦åº¦ 0
            # 'state_num',              # é‡è¦åº¦ 0
            # 'surface_num',            # é‡è¦åº¦ 0
            
            # --- v7 Market Features (é¦¬ã®èƒ½åŠ›ã¨ç„¡é–¢ä¿‚) ---
            'lag1_odds',              # å‰èµ°ã‚ªãƒƒã‚ºï¼ˆå¸‚å ´è©•ä¾¡ï¼‰
            'lag1_popularity',        # å‰èµ°äººæ°—ï¼ˆå¸‚å ´è©•ä¾¡ï¼‰
            
            # --- v11 Speed Index Intermediates (Leakage) ---
            'time_index', 'last_3f_index',
        ]
        # Sample Weights for Odds-Weighted Loss (Phase 15)
        # Use log1p(odds) to prioritize high-value winners without excessive noise sensitivity
        # Default weight = 1.0
        # Winner (Target > 0) weight = 1.0 + np.log1p(odds)
        w = np.ones(len(df))
        if 'odds' in df.columns:
            # fillna(1.0) and use log1p
            odds = df['odds'].fillna(1.0)
            # Apply weight only for Top 3 (target > 0)
            # w[df['target'] > 0] = 1.0 + np.log1p(odds[df['target'] > 0])
            # Wait, log1p of 1.0 is 0.7. log1p of 100 is 4.6.
            # Base weight 1.0. Bonus is log1p(odds).
            is_winner = df['target'] > 0
            w[is_winner] = 1.0 + np.log1p(odds[is_winner])

        # [v09] IDç³»ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å‰Šé™¤)
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Š jockey_id, trainer_id, sire_id ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ã„ãŸã„å ´åˆãŒã‚ã‚‹
        # å®Ÿç¿’æ¸ˆã¿ã® experiment runner ãŒæ˜ç¤ºçš„ã« X ã‚’åŠ å·¥ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã ãŒã€
        # ã“ã“ã§ã¯ exclude_ids=False ãªã‚‰å‰Šé™¤ã—ãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚
        # ä¸€æ—¦ã€ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒªã‚¹ãƒˆã‹ã‚‰å¤–ã™ã€‚
        
        # å­˜åœ¨ã—ãªã„ã‚«ãƒ©ãƒ ã‚’dropã—ã‚ˆã†ã¨ã—ã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã‚ˆã†ã« errors='ignore'
        X = df.drop(columns=drop_cols, errors='ignore')

        # [v09 Fix] jockey_idãªã©ã¯objectå‹ã ãŒCatBoostã§ã¯å¿…è¦ã€‚
        # select_dtypes(exclude=['object']) ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã€
        # å¿…è¦ã«å¿œã˜ã¦ä¸Šä½ã§å‡¦ç†ã™ã‚‹ã€‚
        # X = X.select_dtypes(exclude=['object'])

        y = df['target']

        return {'X': X, 'y': y, 'group': group, 'w': w}
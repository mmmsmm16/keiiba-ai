"""
Hierarchical Feature Pipeline

Phase 16: ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã—ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’å€‹åˆ¥ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€‚
ç‰¹å¾´é‡è¿½åŠ æ™‚ã«å¤‰æ›´ç®‡æ‰€ä»¥é™ã®ã¿å†å‡¦ç†ã™ã‚‹ã“ã¨ã§å‡¦ç†æ™‚é–“ã‚’å¤§å¹…çŸ­ç¸®ã€‚

ä½¿ç”¨ä¾‹:
    pipeline = FeaturePipeline("data/cache/jra")
    pipeline.add_step("raw", loader.load, [])
    pipeline.add_step("cleanse", cleanser.cleanse, ["raw"])
    df = pipeline.run()  # å¿…è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã®ã¿å®Ÿè¡Œ
"""

import os
import hashlib
import json
import pandas as pd
import logging
from datetime import datetime
from typing import Dict, List, Callable, Optional, Any

logger = logging.getLogger(__name__)


class PipelineStep:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®1ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, name: str, processor: Callable, dependencies: List[str], 
                 version: str = "1.0", params: Dict = None):
        """
        Args:
            name: ã‚¹ãƒ†ãƒƒãƒ—åï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨ï¼‰
            processor: å‡¦ç†é–¢æ•°ï¼ˆDataFrameã‚’å¼•æ•°ã¨ã—ã¦å—ã‘å–ã‚Šã€DataFrameã‚’è¿”ã™ï¼‰
            dependencies: ä¾å­˜ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—åã®ãƒªã‚¹ãƒˆ
            version: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆå¤‰æ›´æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ï¼‰
            params: å‡¦ç†é–¢æ•°ã«æ¸¡ã™è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        self.name = name
        self.processor = processor
        self.dependencies = dependencies
        self.version = version
        self.params = params or {}
        self.cache_path: Optional[str] = None
        self.meta_path: Optional[str] = None
    
    def get_cache_key(self, dep_hashes: Dict[str, str]) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆä¾å­˜ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒãƒƒã‚·ãƒ¥ã‚’å«ã‚€ï¼‰"""
        
        def make_serializable(obj):
            """éJSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¤‰æ›"""
            if isinstance(obj, range):
                return list(obj)
            elif isinstance(obj, dict):
                return {k: make_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [make_serializable(item) for item in obj]
            return obj
        
        key_data = {
            "version": self.version,
            "params": make_serializable(self.params),
            "dependencies": {dep: dep_hashes.get(dep, "") for dep in self.dependencies}
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()[:12]


class FeaturePipeline:
    """éšå±¤çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç®¡ç†ã™ã‚‹ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, cache_dir: str, dataset_name: str = "default"):
        """
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè­˜åˆ¥å­ï¼ˆJRA/NARãªã©ï¼‰
        """
        self.cache_dir = os.path.join(cache_dir, dataset_name)
        self.steps: Dict[str, PipelineStep] = {}
        self.execution_order: List[str] = []
        self.step_hashes: Dict[str, str] = {}
        self.cached_data: Dict[str, pd.DataFrame] = {}
        
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def add_step(self, name: str, processor: Callable, dependencies: List[str] = None,
                 version: str = "1.0", params: Dict = None) -> 'FeaturePipeline':
        """
        å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ 
        
        Args:
            name: ã‚¹ãƒ†ãƒƒãƒ—å
            processor: å‡¦ç†é–¢æ•°
            dependencies: ä¾å­˜ã‚¹ãƒ†ãƒƒãƒ—åãƒªã‚¹ãƒˆ
            version: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆå¤‰æ›´æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ï¼‰
            params: è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            selfï¼ˆãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³ç”¨ï¼‰
        """
        step = PipelineStep(name, processor, dependencies or [], version, params)
        step.cache_path = os.path.join(self.cache_dir, f"{name}.parquet")
        step.meta_path = os.path.join(self.cache_dir, f"{name}.meta.json")
        self.steps[name] = step
        
        # ä¾å­˜é–¢ä¿‚ã®æ¤œè¨¼
        for dep in step.dependencies:
            if dep not in self.steps:
                logger.warning(f"Step '{name}' depends on '{dep}' which is not yet defined")
        
        return self
    
    def _resolve_execution_order(self) -> List[str]:
        """ä¾å­˜é–¢ä¿‚ã‚’è§£æ±ºã—ã€å®Ÿè¡Œé †åºã‚’æ±ºå®šï¼ˆãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆï¼‰"""
        visited = set()
        order = []
        
        def visit(name: str):
            if name in visited:
                return
            visited.add(name)
            step = self.steps[name]
            for dep in step.dependencies:
                if dep in self.steps:
                    visit(dep)
            order.append(name)
        
        for name in self.steps:
            visit(name)
        
        return order
    
    def _is_cache_valid(self, step: PipelineStep) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        if not os.path.exists(step.cache_path):
            return False
        if not os.path.exists(step.meta_path):
            return False
        
        try:
            with open(step.meta_path, 'r') as f:
                meta = json.load(f)
            
            # ç¾åœ¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã¨ä¿å­˜ã•ã‚ŒãŸã‚­ãƒ¼ã‚’æ¯”è¼ƒ
            current_key = step.get_cache_key(self.step_hashes)
            return meta.get("cache_key") == current_key
        except Exception as e:
            logger.warning(f"Failed to read metadata for {step.name}: {e}")
            return False
    
    def _save_metadata(self, step: PipelineStep, cache_key: str, row_count: int):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        meta = {
            "cache_key": cache_key,
            "version": step.version,
            "created_at": datetime.now().isoformat(),
            "row_count": row_count,
            "dependencies": step.dependencies
        }
        with open(step.meta_path, 'w') as f:
            json.dump(meta, f, indent=2)
    
    def run(self, force_from: str = None, target_step: str = None) -> pd.DataFrame:
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        
        Args:
            force_from: æŒ‡å®šã—ãŸã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å¼·åˆ¶å†å®Ÿè¡Œ
            target_step: å®Ÿè¡Œã™ã‚‹æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆNoneãªã‚‰å…¨ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
            
        Returns:
            æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›DataFrame
        """
        self.execution_order = self._resolve_execution_order()
        logger.info(f"Pipeline execution order: {' -> '.join(self.execution_order)}")
        
        force_rebuild = False
        
        for step_name in self.execution_order:
            step = self.steps[step_name]
            
            # force_fromæŒ‡å®šæ™‚ã€ãã®ã‚¹ãƒ†ãƒƒãƒ—ä»¥é™ã¯å¼·åˆ¶å†æ§‹ç¯‰
            if force_from and step_name == force_from:
                force_rebuild = True
                logger.info(f"Force rebuild from step: {step_name}")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if not force_rebuild and self._is_cache_valid(step):
                logger.info(f"â­ï¸  Step '{step_name}': Using cached data")
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
                with open(step.meta_path, 'r') as f:
                    meta = json.load(f)
                self.step_hashes[step_name] = meta.get("cache_key", "")
                
                # å¿…è¦ãªå ´åˆã®ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
                if step_name == target_step or step_name == self.execution_order[-1]:
                    self.cached_data[step_name] = pd.read_parquet(step.cache_path)
                continue
            
            # å†æ§‹ç¯‰ãƒ•ãƒ©ã‚°ã‚’ã‚»ãƒƒãƒˆï¼ˆä¾å­˜ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹åŒ–ã•ã‚ŒãŸã‚‰ä»¥é™ã‚‚å†æ§‹ç¯‰ï¼‰
            force_rebuild = True
            
            # ä¾å­˜ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            input_data = None
            if step.dependencies:
                # æœ€åˆã®ä¾å­˜ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨
                main_dep = step.dependencies[0]
                if main_dep in self.cached_data:
                    input_data = self.cached_data[main_dep]
                else:
                    input_data = pd.read_parquet(self.steps[main_dep].cache_path)
            
            # ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ
            logger.info(f"ğŸ”„ Step '{step_name}': Processing...")
            start_time = datetime.now()
            
            if input_data is not None:
                output_data = step.processor(input_data, **step.params)
            else:
                output_data = step.processor(**step.params)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… Step '{step_name}': Completed in {elapsed:.1f}s ({len(output_data):,} rows)")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
            cache_key = step.get_cache_key(self.step_hashes)
            output_data.to_parquet(step.cache_path)
            self._save_metadata(step, cache_key, len(output_data))
            
            self.step_hashes[step_name] = cache_key
            self.cached_data[step_name] = output_data
            
            # target_stepã«åˆ°é”ã—ãŸã‚‰çµ‚äº†
            if target_step and step_name == target_step:
                break
        
        # æœ€çµ‚å‡ºåŠ›ã‚’è¿”ã™
        final_step = target_step or self.execution_order[-1]
        if final_step in self.cached_data:
            return self.cached_data[final_step]
        else:
            return pd.read_parquet(self.steps[final_step].cache_path)
    
    def invalidate(self, step_name: str):
        """æŒ‡å®šã‚¹ãƒ†ãƒƒãƒ—ä»¥é™ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–"""
        if step_name not in self.steps:
            raise ValueError(f"Unknown step: {step_name}")
        
        self.execution_order = self._resolve_execution_order()
        start_idx = self.execution_order.index(step_name)
        
        for name in self.execution_order[start_idx:]:
            step = self.steps[name]
            if os.path.exists(step.cache_path):
                os.remove(step.cache_path)
                logger.info(f"Invalidated cache for step: {name}")
            if os.path.exists(step.meta_path):
                os.remove(step.meta_path)
    
    def get_cache_status(self) -> Dict[str, Dict]:
        """å„ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ã‚’å–å¾—"""
        self.execution_order = self._resolve_execution_order()
        status = {}
        
        for name in self.execution_order:
            step = self.steps[name]
            if os.path.exists(step.meta_path):
                with open(step.meta_path, 'r') as f:
                    meta = json.load(f)
                status[name] = {
                    "cached": True,
                    "created_at": meta.get("created_at"),
                    "row_count": meta.get("row_count"),
                    "version": meta.get("version")
                }
            else:
                status[name] = {"cached": False}
        
        return status
    
    def print_status(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ã‚’è¡¨ç¤º"""
        status = self.get_cache_status()
        print("\n=== Pipeline Cache Status ===")
        for name, info in status.items():
            if info["cached"]:
                print(f"  âœ… {name}: {info['row_count']:,} rows (v{info['version']}, {info['created_at'][:10]})")
            else:
                print(f"  âŒ {name}: No cache")
        print()

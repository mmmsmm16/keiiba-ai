import os
import pickle
import logging
import json
import pandas as pd
import numpy as np
from src.pipeline.config import ExperimentConfig
from src.model.lgbm import KeibaLGBM
from src.model.catboost_model import KeibaCatBoost
from src.model.tabnet_model import KeibaTabNet
from src.model.ensemble import EnsembleModel

logger = logging.getLogger(__name__)

def load_datasets(run_dir: str):
    dataset_path = os.path.join(run_dir, "data/lgbm_datasets.pkl")
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"Dataset not found at {dataset_path}")
    
    with open(dataset_path, 'rb') as f:
        datasets = pickle.load(f)
    return datasets['train'], datasets['valid']

def train_lgbm(train_set, valid_set, params, run_dir):
    logger.info("âš¡ Training LightGBM...")
    model = KeibaLGBM(params=params)
    model.train(train_set, valid_set)
    
    output_path = os.path.join(run_dir, "models/lgbm.pkl")
    model.save_model(output_path)
    model.plot_importance(os.path.join(run_dir, "reports/lgbm_importance.png"))
    return model

def train_catboost(train_set, valid_set, params, run_dir):
    logger.info("ğŸ± Training CatBoost...")
    model = KeibaCatBoost(params=params)
    model.train(train_set, valid_set)
    
    output_path = os.path.join(run_dir, "models/catboost.pkl")
    model.save_model(output_path)
    return model

def train_tabnet(train_set, valid_set, params, run_dir):
    logger.info("ğŸ•¸ï¸ Training TabNet...")
    
    # GPUç«¶åˆå¯¾ç­–: CUDAçŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
    # CatBoost/LightGBMã®å¾Œã«PyTorchã‚’ä½¿ã†å ´åˆã€CUDAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒç ´æã™ã‚‹ã“ã¨ãŒã‚ã‚‹
    import torch
    if torch.cuda.is_available():
        logger.info("ğŸ”„ CUDAçŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã„ã¾ã™...")
        torch.cuda.empty_cache()
        # cuBLASãƒãƒ³ãƒ‰ãƒ«ã‚’å¼·åˆ¶çš„ã«å†åˆæœŸåŒ–
        try:
            # ãƒ€ãƒŸãƒ¼æ¼”ç®—ã§cuBLASã‚’åˆæœŸåŒ–
            dummy = torch.randn(10, 10, device='cuda')
            _ = torch.matmul(dummy, dummy)
            del dummy
            torch.cuda.empty_cache()
            logger.info("âœ… CUDAåˆæœŸåŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ CUDAåˆæœŸåŒ–å¤±æ•— - CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
            params = params.copy() if params else {}
            params['device_name'] = 'cpu'
    
    model = KeibaTabNet(params=params)
    model.train(train_set, valid_set)
    
    output_path = os.path.join(run_dir, "models/tabnet.zip")
    model.save_model(output_path)
    return model

def train_ensemble(train_set, valid_set, run_dir):
    logger.info("ğŸ¤ Training Ensemble Meta Model...")
    
    model = EnsembleModel()
    models_dir = os.path.join(run_dir, "models")
    
    # Check for base models. At least LGBM and CatBoost are expected for standard ensemble.
    # If TabNet exists, it will be loaded automatically by EnsembleModel if it looks for it.
    # Let's verify EnsembleModel behavior. Typically it loads lgbm.pkl, catboost.pkl, tabnet.zip if present.
    
    if not (os.path.exists(os.path.join(models_dir, "lgbm.pkl")) and 
            os.path.exists(os.path.join(models_dir, "catboost.pkl"))):
        raise RuntimeError("Base models (lgbm, catboost) missing for ensemble.")

    model.load_base_models(models_dir, version=None) 
    
    model.train_meta_model(valid_set)
    
    output_path = os.path.join(run_dir, "models/ensemble.pkl")
    model.save_model(output_path)
    return model

def train_roi_model(train_set, valid_set, params, run_dir, raw_data_path: str = None):
    """
    ROIæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    
    Args:
        train_set: {'X': DataFrame, 'y': Series, 'group': Array}
        valid_set: {'X': DataFrame, 'y': Series, 'group': Array}
        params: ROIãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        run_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        raw_data_path: ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆodds, rankå«ã‚€ï¼‰ã¸ã®ãƒ‘ã‚¹
    """
    logger.info("ğŸ’° Training ROI Model...")
    
    import torch
    from torch.utils.data import Dataset, DataLoader
    from torch.optim import AdamW
    from torch.optim.lr_scheduler import CosineAnnealingLR
    from src.model.roi_model import ROIModel, RaceWinPredictor
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    params = params or {}
    model_type = params.get('model_type', 'simple')
    loss_type = params.get('loss_type', 'evmax')
    hidden_dim = params.get('hidden_dim', 128)
    num_layers = params.get('num_layers', 2)
    dropout = params.get('dropout', 0.3)
    epochs = params.get('epochs', 30)
    batch_size = params.get('batch_size', 64)
    lr = params.get('lr', 1e-3)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"Using device: {device}")
    
    # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆodds, rankæƒ…å ±ãŒå¿…è¦ï¼‰
    if raw_data_path is None:
        raw_data_path = os.path.join(run_dir, "data/preprocessed_data.parquet")
    
    if not os.path.exists(raw_data_path):
        logger.warning(f"Raw data not found at {raw_data_path}. ROI model requires odds data.")
        return None
    
    raw_df = pd.read_parquet(raw_data_path)
    
    # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ 
    feature_cols = train_set['X'].columns.tolist()
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆãƒ¬ãƒ¼ã‚¹å˜ä½ã§ãƒãƒƒãƒåŒ–ï¼‰
    class RaceDataset(Dataset):
        def __init__(self, df, feature_cols, max_horses=18):
            self.max_horses = max_horses
            self.races = []
            
            # æ•°å€¤å‹ã‚«ãƒ©ãƒ ã®ã¿ã‚’ä½¿ç”¨ (æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã‚’é™¤å¤–)
            numeric_df = df[feature_cols].select_dtypes(include=[np.number])
            self.feature_cols = numeric_df.columns.tolist()
            logger.info(f"RaceDataset: Using {len(self.feature_cols)} numeric features (excluded non-numeric)")
            
            for race_id, grp in df.groupby('race_id'):
                if len(grp) < 3:
                    continue
                grp = grp.sort_values('horse_number')
                
                # æ•°å€¤å‹ã‚«ãƒ©ãƒ ã®ã¿å–å¾—
                X = grp[self.feature_cols].values.astype(np.float32)
                # NaN ã‚’ 0 ã§åŸ‹ã‚ã‚‹
                X = np.nan_to_num(X, nan=0.0)
                
                ranks = grp['rank'].values
                is_winner = (ranks == 1).astype(np.float32)
                odds = grp['odds'].fillna(1.0).values.astype(np.float32)
                
                self.races.append({
                    'X': X, 'is_winner': is_winner, 'odds': odds, 'n_horses': len(grp)
                })
        
        def __len__(self):
            return len(self.races)
        
        def __getitem__(self, idx):
            race = self.races[idx]
            n = min(race['n_horses'], self.max_horses)  # max_horsesã§truncate
            
            X_padded = np.zeros((self.max_horses, len(self.feature_cols)), dtype=np.float32)
            is_winner_padded = np.zeros(self.max_horses, dtype=np.float32)
            odds_padded = np.ones(self.max_horses, dtype=np.float32)
            mask = np.zeros(self.max_horses, dtype=np.float32)
            
            X_padded[:n] = race['X'][:n]  # max_horsesåˆ†ã®ã¿ä½¿ç”¨
            is_winner_padded[:n] = race['is_winner'][:n]
            odds_padded[:n] = race['odds'][:n]
            mask[:n] = 1.0
            
            return {
                'X': torch.tensor(X_padded),
                'is_winner': torch.tensor(is_winner_padded),
                'odds': torch.tensor(odds_padded),
                'mask': torch.tensor(mask)
            }
    
    # ã‚«ãƒ©ãƒ ç¢ºèªãƒ»è£œå®Œ
    for c in feature_cols:
        if c not in raw_df.columns:
            raw_df[c] = 0
    
    raw_df['rank'] = pd.to_numeric(raw_df['rank'], errors='coerce')
    raw_df['odds'] = pd.to_numeric(raw_df['odds'], errors='coerce').fillna(1.0)
    raw_df = raw_df.dropna(subset=['rank'])
    
    # Train/Validåˆ†å‰²
    train_years = raw_df['year'].min()  # å®Ÿéš›ã¯configã‹ã‚‰å–å¾—ã™ã¹ã
    valid_year = raw_df['year'].max()
    train_df = raw_df[raw_df['year'] < valid_year].copy()
    valid_df = raw_df[raw_df['year'] == valid_year].copy()
    
    logger.info(f"Train: {len(train_df)} rows, Valid: {len(valid_df)} rows")
    
    train_dataset = RaceDataset(train_df, feature_cols)
    valid_dataset = RaceDataset(valid_df, feature_cols)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆRaceDatasetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸç‰¹å¾´é‡æ•°ã‚’ä½¿ç”¨ï¼‰
    actual_feature_count = len(train_dataset.feature_cols)
    model = ROIModel(
        model_type=model_type, 
        hidden_dim=hidden_dim, 
        num_layers=num_layers,
        dropout=dropout,
        device=device
    )
    model.build_model(actual_feature_count)
    
    # æå¤±é–¢æ•°
    from src.model.roi_loss import (
        EVMaxLoss, OddsWeightedBCE, ROIProxyLoss, 
        CombinedROILoss, AccuracyROILoss, RankingLoss
    )
    
    # æå¤±é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    loss_params = params.get('loss_params', {})
    
    if loss_type == 'evmax':
        criterion = EVMaxLoss(**loss_params)
    elif loss_type == 'odds_bce':
        criterion = OddsWeightedBCE(**loss_params)
    elif loss_type == 'roi_proxy':
        criterion = ROIProxyLoss(**loss_params)
    elif loss_type == 'combined':
        criterion = CombinedROILoss(**loss_params)
    elif loss_type == 'accuracy_roi':
        # AccuracyROILoss(accuracy_weight=0.7, roi_weight=0.3)
        criterion = AccuracyROILoss(**loss_params)
    elif loss_type == 'ranking':
        criterion = RankingLoss(**loss_params)
    else:
        criterion = EVMaxLoss()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    logger.info(f"Using loss function: {loss_type} ({type(criterion).__name__})")
    logger.info(f"Loss params: {loss_params}")
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    optimizer = AdamW(model.model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    best_roi = 0
    patience = params.get('patience', 20)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ20 epoch
    patience_counter = 0
    
    logger.info(f"Early stopping patience: {patience}")
    
    for epoch in range(1, epochs + 1):
        # Train
        model.model.train()
        total_loss = 0
        n_batches = 0
        
        for batch in train_loader:
            X = batch['X'].to(device)
            is_winner = batch['is_winner'].to(device)
            odds = batch['odds'].to(device)
            mask = batch['mask'].to(device)
            
            optimizer.zero_grad()
            probs = model.model(X, mask)
            loss = criterion(probs, is_winner, odds, mask)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            n_batches += 1
        
        scheduler.step()
        
        # Eval
        model.model.eval()
        total_cost = 0
        total_return = 0
        n_hits = 0
        n_races = 0
        
        with torch.no_grad():
            for batch in valid_loader:
                X = batch['X'].to(device)
                is_winner = batch['is_winner'].to(device)
                odds = batch['odds'].to(device)
                mask = batch['mask'].to(device)
                
                probs = model.model(X, mask)
                
                batch_size_cur = X.shape[0]
                for i in range(batch_size_cur):
                    valid_mask = mask[i].bool()
                    valid_probs = probs[i][valid_mask]
                    valid_winner = is_winner[i][valid_mask]
                    valid_odds = odds[i][valid_mask]
                    
                    if len(valid_probs) == 0:
                        continue
                    
                    top1_idx = valid_probs.argmax()
                    is_hit = valid_winner[top1_idx].item() == 1
                    
                    total_cost += 100
                    if is_hit:
                        total_return += valid_odds[top1_idx].item() * 100
                        n_hits += 1
                    n_races += 1
        
        roi = (total_return / total_cost * 100) if total_cost > 0 else 0
        acc = (n_hits / n_races * 100) if n_races > 0 else 0
        
        if epoch % 5 == 0 or epoch == 1:
            logger.info(f"Epoch {epoch:3d} | Loss: {total_loss/n_batches:.4f} | ROI: {roi:.1f}% | Acc: {acc:.1f}%")
        
        if roi > best_roi:
            best_roi = roi
            patience_counter = 0  # ãƒªã‚»ãƒƒãƒˆ
            model.save(os.path.join(run_dir, 'models', 'roi_model_best.pt'))
            logger.info(f"New best ROI! Model saved.")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"Early stopping triggered at epoch {epoch} (Best ROI: {best_roi:.1f}%)")
                break
    
    logger.info(f"âœ… ROI Model Training Completed. Best ROI: {best_roi:.1f}%")
    return model

def train_model(config: ExperimentConfig, run_dir: str):
    train_set, valid_set = load_datasets(run_dir)
    
    # ç‰¹å¾´é‡å‰Šé™¤ (Configåˆ¶å¾¡)
    if config.data.drop_features:
        logger.info(f"æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡ã‚’å‰Šé™¤ã—ã¾ã™: {config.data.drop_features}")
        for ds in [train_set, valid_set]:
            if 'X' in ds and isinstance(ds['X'], pd.DataFrame):
                # å­˜åœ¨ã—ãªã„ã‚«ãƒ©ãƒ ã¯ç„¡è¦– (errors='ignore')
                ds['X'] = ds['X'].drop(columns=config.data.drop_features, errors='ignore')
    
    model_type = config.model.type
    
    trained_models = {}
    
    # ensemble_only: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®ã¿å­¦ç¿’
    run_lgbm = (model_type == 'lgbm' or model_type == 'ensemble')
    run_catboost = (model_type == 'catboost' or model_type == 'ensemble')
    
    # TabNetå®Ÿè¡Œåˆ¤å®š: tabnetå˜ä½“æŒ‡å®š or ensembleæŒ‡å®š
    # tabnet_params.enabled == false ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    tabnet_enabled = True
    if config.model.tabnet_params and config.model.tabnet_params.get('enabled') is False:
        tabnet_enabled = False
        logger.info("â­ï¸ TabNetã¯skipã•ã‚Œã¾ã™ (enabled: false)")
    
    run_tabnet = (model_type == 'tabnet' and tabnet_enabled)
    if model_type == 'ensemble' and tabnet_enabled:
         run_tabnet = True

    run_ensemble = (model_type == 'ensemble' or model_type == 'ensemble_only')
    
    # ensemble_only ã®å ´åˆã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãŸã ã—ã€å­˜åœ¨ã—ãªã„æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ã¯å­¦ç¿’ã™ã‚‹ï¼‰
    if model_type == 'ensemble_only':
        models_dir = os.path.join(run_dir, "models")
        logger.info("ğŸš€ ensemble_only ãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªã—ã€ä¸è¶³åˆ†ã®ã¿å­¦ç¿’ã—ã¾ã™")
        
        # LightGBM: å¸¸ã«å¿…è¦
        if os.path.exists(os.path.join(models_dir, "lgbm.pkl")):
            run_lgbm = False
            logger.info("  âœ… LightGBM: æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
        else:
            run_lgbm = True
            logger.info("  ğŸ”§ LightGBM: ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - å­¦ç¿’ã—ã¾ã™")
        
        # CatBoost: å¸¸ã«å¿…è¦
        if os.path.exists(os.path.join(models_dir, "catboost.pkl")):
            run_catboost = False
            logger.info("  âœ… CatBoost: æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
        else:
            run_catboost = True
            logger.info("  ğŸ”§ CatBoost: ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - å­¦ç¿’ã—ã¾ã™")
        
        # TabNet: enabled=trueã®å ´åˆã®ã¿ç¢ºèª
        if tabnet_enabled:
            if os.path.exists(os.path.join(models_dir, "tabnet.zip")):
                run_tabnet = False
                logger.info("  âœ… TabNet: æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
            else:
                run_tabnet = True
                logger.info("  ğŸ”§ TabNet: ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - å­¦ç¿’ã—ã¾ã™")
        else:
            run_tabnet = False
            logger.info("  â­ï¸ TabNet: ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")

    if run_lgbm:
        lgbm = train_lgbm(train_set, valid_set, config.model.lgbm_params, run_dir)
        trained_models['lgbm'] = lgbm
        
    if run_catboost:
        cat = train_catboost(train_set, valid_set, config.model.catboost_params, run_dir)
        trained_models['catboost'] = cat
        
    if run_tabnet:
        # TabNet params default if None
        t_params = config.model.tabnet_params if config.model.tabnet_params else {}
        tab = train_tabnet(train_set, valid_set, t_params, run_dir)
        trained_models['tabnet'] = tab
        
    if run_ensemble:
        ens = train_ensemble(train_set, valid_set, run_dir)
        trained_models['ensemble'] = ens
    
    # ROIãƒ¢ãƒ‡ãƒ«
    if model_type == 'roi':
        logger.info("ğŸ’° ROI Model Training Mode")
        roi_model = train_roi_model(train_set, valid_set, config.model.roi_params, run_dir)
        trained_models['roi'] = roi_model
        
    logger.info("âœ… Model Training Completed.")

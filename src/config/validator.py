
import logging
import re
import os
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)

class ConfigValidator:
    """
    å®Ÿé¨“è¨­å®š(Config)ã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    å®Ÿé¨“åã¨Targetã®ä¸ä¸€è‡´ãªã©ã‚’æ¤œå‡ºã—ã€äº‹æ•…ã‚’é˜²æ­¢ã™ã‚‹ã€‚
    """
    
    @staticmethod
    def validate(config: Dict[str, Any], config_path: str = None, strict: bool = False) -> None:
        """
        Configã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
        å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ ValueError ã‚’é€å‡ºã™ã‚‹ã€‚
        
        Args:
            config: ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®Configè¾æ›¸
            config_path: Configãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒ•ã‚¡ã‚¤ãƒ«åãƒã‚§ãƒƒã‚¯ç”¨)
            strict: Trueã®å ´åˆã€Warningãƒ¬ãƒ™ãƒ«ã®ä¸æ•´åˆã‚‚Errorã¨ã—ã¦æ‰±ã†
        """
        logger.info(f"ğŸ›¡ï¸ Config Guardrail: Validating experiment configuration... (Strict={strict})")
        
        exp_name = config.get('experiment_name', config.get('name', 'unknown'))
        dataset = config.get('dataset', {})
        model_params = config.get('model_params', {})
        objective = model_params.get('objective', 'binary')
        
        # 1. Target Consistency Check (Experiment Name & Filename)
        ConfigValidator._validate_target_consistency(exp_name, dataset, objective, config_path)
        
        # 2. Metric Consistency Check
        ConfigValidator._validate_metric_consistency(model_params, dataset, strict)
        
        # 3. Time-Decay Consistency Check
        ConfigValidator._validate_time_decay_consistency(config.get('sample_weight', {}), strict)
        
        # 4. Group Key Consistency Check
        ConfigValidator._validate_group_key_consistency(model_params, objective, strict)

        # 5. Task vs Objective Check
        ConfigValidator._validate_task_objective_consistency(config, strict)
        
        logger.info("âœ… Config Guardrail: Validation Passed.")

    @staticmethod
    def _validate_target_consistency(exp_name: str, dataset: Dict[str, Any], objective: str, config_path: str = None) -> None:
        """å®Ÿé¨“åã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ ã®çŸ›ç›¾ã‚’ãƒã‚§ãƒƒã‚¯"""
        target_col = dataset.get('target_col', '')
        binary_target = dataset.get('binary_target', '')
        
        # Check both Experiment Name and Config Filename
        names_to_check = [exp_name]
        if config_path:
            basename = os.path.basename(config_path)
            names_to_check.append(basename)
            
        for name in names_to_check:
            name_lower = name.lower()
            
            # Rule: "top3" in name => target must be Top3
            if 'top3' in name_lower:
                if target_col and target_col != 'target_top3':
                    if not binary_target or binary_target != 'top3':
                        raise ValueError(f"â›” CONFIG ERROR: Name '{name}' contains 'top3' but target is '{target_col}' (binary_target='{binary_target}'). Expected 'target_top3'.")
                elif not target_col and (not binary_target or binary_target != 'top3'):
                    raise ValueError(f"â›” CONFIG ERROR: Name '{name}' contains 'top3' but binary_target is '{binary_target}'. Expected 'top3'.")

            # Rule: "top2" in name => target must be Top2
            if 'top2' in name_lower:
                if target_col and target_col != 'target_top2':
                    if not binary_target or binary_target != 'top2':
                        raise ValueError(f"â›” CONFIG ERROR: Name '{name}' contains 'top2' but target is '{target_col}' (binary_target='{binary_target}'). Expected 'target_top2'.")
                elif not target_col and (not binary_target or binary_target != 'top2'):
                    raise ValueError(f"â›” CONFIG ERROR: Name '{name}' contains 'top2' but binary_target is '{binary_target}'. Expected 'top2'.")

            # Rule: "win" or "top1" in name => target must be Win
            if 'win' in name_lower or 'top1' in name_lower:
                if 'winter' in name_lower or 'twin' in name_lower: continue # basic exclusion
                if target_col and target_col != 'target_win':
                    if not binary_target or binary_target != 'win':
                        raise ValueError(f"â›” CONFIG ERROR: Name '{name}' contains 'win' but target is '{target_col}' (binary_target='{binary_target}'). Expected 'target_win'.")
                elif not target_col and (binary_target and binary_target != 'win'):
                    raise ValueError(f"â›” CONFIG ERROR: Name '{name}' contains 'win' but binary_target is '{binary_target}'. Expected 'win'.")

    @staticmethod
    def _validate_metric_consistency(model_params: Dict[str, Any], dataset: Dict[str, Any], strict: bool) -> None:
        """æŒ‡æ¨™ã¨ã‚¿ã‚¹ã‚¯ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        objective = model_params.get('objective', '')
        metrics = model_params.get('metric', [])
        if isinstance(metrics, str): metrics = [metrics]
        
        msg = ""
        if objective == 'lambdarank':
            if not any(m.lower().startswith('ndcg') or m.lower().startswith('map') for m in metrics):
                msg = "LambdaRank objective but no NDCG/MAP metric specified."
        
        elif objective == 'binary':
            if not any(m.lower() in ['auc', 'binary_logloss', 'logloss'] for m in metrics):
                msg = "Binary objective but no AUC/LogLoss metric specified."

        if msg:
            if strict:
                raise ValueError(f"â›” CONFIG ERROR (Strict): {msg}")
            else:
                logger.warning(f"âš ï¸ GUARDRAIL WARN: {msg}")

    @staticmethod
    def _validate_time_decay_consistency(sample_weight: Dict[str, Any], strict: bool) -> None:
        """Time-Decayè¨­å®šã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        if not sample_weight.get('enabled', False):
            return

        strategy = sample_weight.get('strategy', 'none')
        
        if strategy == 'piecewise':
            yw = sample_weight.get('year_weights', {})
            if not yw:
                 raise ValueError("â›” CONFIG ERROR: Strategy 'piecewise' requires 'year_weights' map.")
            for k, w in yw.items():
                if w <= 0 or w > 1.0:
                    msg = f"Unusual weight value {w} for key {k}. Usually 0 < w <= 1."
                    if strict: raise ValueError(f"â›” CONFIG ERROR (Strict): {msg}")
                    else: logger.warning(f"âš ï¸ GUARDRAIL WARN: {msg}")
                    
        elif strategy == 'exponential':
            decay = sample_weight.get('decay_rate', 0.0)
            if decay <= 0:
                 raise ValueError(f"â›” CONFIG ERROR: Strategy 'exponential' requires positive 'decay_rate'. Found {decay}.")

    @staticmethod
    def _validate_group_key_consistency(model_params: Dict[str, Any], objective: str, strict: bool) -> None:
        """Rankingã‚¿ã‚¹ã‚¯ã§ã®GroupKeyæ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        pass

    @staticmethod
    def _validate_task_objective_consistency(config: Dict[str, Any], strict: bool) -> None:
        """TaskTypeã¨Objectiveã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        task_type = config.get('task_type', '') # Optional key
        objective = config.get('model_params', {}).get('objective', '')
        
        if not task_type: return

        if task_type == 'ranking':
            if objective not in ['lambdarank', 'rank:pairwise', 'rank:ndcg', 'yetirank']:
                raise ValueError(f"â›” CONFIG ERROR: task_type='ranking' but objective='{objective}'. Expected ranking objective.")
        
        elif task_type == 'classification':
             if objective not in ['binary', 'multiclass', 'cross_entropy', 'logloss']:
                 raise ValueError(f"â›” CONFIG ERROR: task_type='classification' but objective='{objective}'. Expected binary/multiclass objective.")

"""
Phase T4: Feature Consistency Verification Script
=================================================
Compares features generated by JIT pipeline vs Training pipeline
to ensure consistency between production and training environments.

Usage:
    docker compose exec app python scripts/adhoc/verify_jit_features.py --date 20241115
"""

import os
import sys
import argparse
import logging
import pandas as pd
import numpy as np
import shutil
from datetime import datetime, timedelta

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from src.preprocessing.loader import JraVanDataLoader
from src.preprocessing.feature_pipeline import FeaturePipeline

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# JIT blocks that are computed in production_run_t2_jit.py
JIT_BLOCKS = [
    'base_attributes',
    'history_stats',
    'jockey_stats',
    'pace_stats',
    'pace_pressure_stats',
    'relative_stats',
    'jockey_trainer_stats',
    'bloodline_stats',
    'training_stats',
    'burden_stats',
    'changes_stats',
    'aptitude_stats',
    'speed_index_stats',
    'class_stats',
    'runstyle_fit',
    'risk_stats'
]


# Tolerance for floating-point comparison
TOLERANCE = 1e-6


def generate_jit_features(loader: JraVanDataLoader, target_date: str, horse_ids: list) -> pd.DataFrame:
    """
    Simulate JIT feature generation for a past date.
    This mimics the logic in production_run_t2_jit.py
    """
    logger.info(f"[JIT Mode] Generating features for {target_date}")
    
    t_year = target_date[:4]
    t_mmdd = target_date[4:]
    date_str = f"{t_year}-{t_mmdd[:2]}-{t_mmdd[2:]}"
    
    # Load today's races
    df_today = loader.load(history_start_date=date_str, end_date=date_str)
    if df_today.empty:
        logger.error("No races found for JIT mode")
        return pd.DataFrame()
    
    if horse_ids:
        df_today = df_today[df_today['horse_id'].astype(str).isin([str(h) for h in horse_ids])]
    
    jit_horse_ids = df_today['horse_id'].astype(str).unique().tolist()
    logger.info(f"[JIT] Found {len(jit_horse_ids)} horses")
    
    # Load horse history (past 12 years)
    history_end = (pd.to_datetime(date_str) - timedelta(days=1)).strftime('%Y-%m-%d')
    df_history = loader.load(
        history_start_date="2014-01-01",
        end_date=history_end,
        horse_ids=jit_horse_ids,
        skip_odds=True
    )
    logger.info(f"[JIT] Loaded {len(df_history)} history records")
    
    # Combine
    df_total = pd.concat([df_history, df_today], ignore_index=True)
    df_total.drop_duplicates(subset=['race_id', 'horse_number'], inplace=True)
    df_total.sort_values(['horse_id', 'date'], inplace=True)
    
    # Ensure datatypes
    for c in ['horse_id', 'jockey_id', 'trainer_id', 'sire_id']:
        if c in df_total.columns:
            df_total[c] = df_total[c].astype(str).str.strip()
    if 'date' in df_total.columns:
        df_total['date'] = pd.to_datetime(df_total['date'])
    
    # Compute features using temp cache
    temp_cache = "data/jit_verify_temp"
    if os.path.exists(temp_cache):
        shutil.rmtree(temp_cache)
    
    pipeline = FeaturePipeline(cache_dir=temp_cache)
    
    try:
        df_jit_feats = pipeline.load_features(df_total, JIT_BLOCKS)
    finally:
        if os.path.exists(temp_cache):
            shutil.rmtree(temp_cache)
    
    # Filter for target date only
    target_race_ids = df_today['race_id'].unique()
    df_jit_feats = df_jit_feats[df_jit_feats['race_id'].isin(target_race_ids)].copy()
    
    return df_jit_feats


def generate_training_features(loader: JraVanDataLoader, target_date: str) -> pd.DataFrame:
    """
    Simulate Training-time feature generation.
    This mimics the logic in run_t2_step1_features.py
    """
    logger.info(f"[Training Mode] Generating features for {target_date}")
    
    t_year = target_date[:4]
    t_mmdd = target_date[4:]
    date_str = f"{t_year}-{t_mmdd[:2]}-{t_mmdd[2:]}"
    
    # Load full historical data (larger window to ensure consistent statistics)
    start_date = "2015-01-01"
    
    df = loader.load(history_start_date=start_date, end_date=date_str, skip_training=False)
    if df.empty:
        logger.error("No data found for Training mode")
        return pd.DataFrame()
    
    logger.info(f"[Training] Loaded {len(df)} total records")
    
    # Use a dedicated cache for training features
    train_cache = "data/train_verify_temp"
    if os.path.exists(train_cache):
        shutil.rmtree(train_cache)
    
    pipeline = FeaturePipeline(cache_dir=train_cache)
    
    try:
        df_train_feats = pipeline.load_features(df, JIT_BLOCKS)
    finally:
        if os.path.exists(train_cache):
            shutil.rmtree(train_cache)
    
    # Filter for target date only
    df['date'] = pd.to_datetime(df['date'])
    target_race_ids = df[df['date'] == date_str]['race_id'].unique()
    df_train_feats = df_train_feats[df_train_feats['race_id'].isin(target_race_ids)].copy()
    
    return df_train_feats


def compare_features(df_jit: pd.DataFrame, df_train: pd.DataFrame) -> dict:
    """
    Compare features from JIT and Training pipelines.
    Returns a detailed report.
    """
    logger.info("Comparing JIT vs Training features...")
    
    # Merge on common keys
    keys = ['race_id', 'horse_number']
    df_merged = pd.merge(
        df_jit, df_train, 
        on=keys, 
        suffixes=('_jit', '_train'),
        how='inner'
    )
    
    logger.info(f"Merged records: {len(df_merged)}")
    
    if df_merged.empty:
        return {"error": "No matching records found"}
    
    # Find common feature columns
    jit_cols = set([c.replace('_jit', '') for c in df_merged.columns if c.endswith('_jit')])
    train_cols = set([c.replace('_train', '') for c in df_merged.columns if c.endswith('_train')])
    common_cols = jit_cols.intersection(train_cols)
    
    report = {
        "total_records": len(df_merged),
        "features_compared": len(common_cols),
        "feature_details": {},
        "nan_counts": {},
        "inf_counts": {},
        "mismatches": [],
        "summary": {}
    }
    
    total_exact_matches = 0
    total_comparisons = 0
    
    for col in sorted(common_cols):
        jit_col = f"{col}_jit"
        train_col = f"{col}_train"
        
        if jit_col not in df_merged.columns or train_col not in df_merged.columns:
            continue
        
        jit_vals = df_merged[jit_col]
        train_vals = df_merged[train_col]
        
        # Skip non-numeric columns
        if not pd.api.types.is_numeric_dtype(jit_vals) or not pd.api.types.is_numeric_dtype(train_vals):
            # For categorical, do exact string comparison
            exact_match = (jit_vals.astype(str) == train_vals.astype(str)).sum()
            match_rate = exact_match / len(df_merged)
            report["feature_details"][col] = {
                "type": "categorical",
                "match_rate": match_rate,
                "exact_matches": int(exact_match)
            }
            total_exact_matches += exact_match
            total_comparisons += len(df_merged)
            continue
        
        # NaN/Inf checks
        nan_jit = jit_vals.isna().sum()
        nan_train = train_vals.isna().sum()
        inf_jit = np.isinf(jit_vals.replace([np.inf, -np.inf], np.nan).dropna()).sum() if jit_vals.dtype in [np.float64, np.float32] else 0
        inf_train = np.isinf(train_vals.replace([np.inf, -np.inf], np.nan).dropna()).sum() if train_vals.dtype in [np.float64, np.float32] else 0
        
        report["nan_counts"][col] = {"jit": int(nan_jit), "train": int(nan_train)}
        report["inf_counts"][col] = {"jit": int(inf_jit), "train": int(inf_train)}
        
        # Difference calculation
        # Handle NaN: both NaN = match, one NaN = mismatch
        both_valid = ~(jit_vals.isna() | train_vals.isna())
        
        if both_valid.sum() == 0:
            report["feature_details"][col] = {
                "type": "numeric",
                "match_rate": 0.0,
                "mean_abs_diff": np.nan,
                "max_abs_diff": np.nan,
                "note": "All values are NaN"
            }
            continue
        
        diff = (jit_vals[both_valid] - train_vals[both_valid]).abs()
        exact_match = (diff < TOLERANCE).sum()
        match_rate = exact_match / len(df_merged)
        
        mean_diff = diff.mean()
        max_diff = diff.max()
        
        report["feature_details"][col] = {
            "type": "numeric",
            "match_rate": match_rate,
            "exact_matches": int(exact_match),
            "mean_abs_diff": float(mean_diff),
            "max_abs_diff": float(max_diff),
            "std_diff": float(diff.std())
        }
        
        total_exact_matches += exact_match
        total_comparisons += len(df_merged)
        
        # Record mismatches for investigation
        if match_rate < 1.0:
            mismatch_idx = diff[diff >= TOLERANCE].head(3).index.tolist()
            for idx in mismatch_idx:
                report["mismatches"].append({
                    "feature": col,
                    "race_id": df_merged.loc[idx, 'race_id'],
                    "horse_number": df_merged.loc[idx, 'horse_number'],
                    "jit_value": float(jit_vals.loc[idx]) if not pd.isna(jit_vals.loc[idx]) else None,
                    "train_value": float(train_vals.loc[idx]) if not pd.isna(train_vals.loc[idx]) else None
                })
    
    # Summary
    report["summary"] = {
        "overall_match_rate": total_exact_matches / total_comparisons if total_comparisons > 0 else 0,
        "total_exact_matches": total_exact_matches,
        "total_comparisons": total_comparisons,
        "features_with_100_match": sum(1 for d in report["feature_details"].values() if d.get("match_rate", 0) == 1.0),
        "features_with_issues": sum(1 for d in report["feature_details"].values() if d.get("match_rate", 1.0) < 1.0)
    }
    
    return report


def sanity_check_features(df: pd.DataFrame, label: str) -> dict:
    """
    Perform sanity checks on features.
    """
    logger.info(f"[{label}] Running sanity checks...")
    
    results = {
        "total_records": len(df),
        "nan_summary": {},
        "inf_summary": {},
        "outlier_summary": {},
        "issues": []
    }
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    for col in numeric_cols:
        vals = df[col]
        
        # NaN check
        nan_count = vals.isna().sum()
        nan_pct = nan_count / len(df) * 100
        
        if nan_pct > 50:
            results["issues"].append(f"{col}: {nan_pct:.1f}% NaN values")
        results["nan_summary"][col] = {"count": int(nan_count), "pct": nan_pct}
        
        # Infinity check
        inf_count = np.isinf(vals).sum()
        if inf_count > 0:
            results["issues"].append(f"{col}: {inf_count} Infinity values")
        results["inf_summary"][col] = int(inf_count)
        
        # Outlier check (values beyond 10 std from mean)
        valid_vals = vals.dropna()
        if len(valid_vals) > 0:
            mean = valid_vals.mean()
            std = valid_vals.std()
            if std > 0:
                z_scores = (valid_vals - mean) / std
                outliers = (z_scores.abs() > 10).sum()
                if outliers > 0:
                    results["outlier_summary"][col] = int(outliers)
    
    return results


def print_report(report: dict, sanity_jit: dict, sanity_train: dict):
    """
    Print formatted verification report.
    """
    print("\n" + "=" * 70)
    print(" Phase T4: Feature Consistency Verification Report")
    print("=" * 70)
    
    print(f"\nüìä Summary:")
    print(f"   Total Records Compared: {report.get('total_records', 0)}")
    print(f"   Features Compared: {report.get('features_compared', 0)}")
    
    summary = report.get("summary", {})
    print(f"\n   Overall Match Rate: {summary.get('overall_match_rate', 0):.2%}")
    print(f"   Features with 100% Match: {summary.get('features_with_100_match', 0)}")
    print(f"   Features with Issues: {summary.get('features_with_issues', 0)}")
    
    # Feature details
    print("\nüìã Feature-Level Results:")
    print("-" * 70)
    print(f"{'Feature':<30} {'Match Rate':>12} {'Mean Diff':>12} {'Max Diff':>12}")
    print("-" * 70)
    
    for feat, details in sorted(report.get("feature_details", {}).items()):
        match_rate = details.get("match_rate", 0)
        mean_diff = details.get("mean_abs_diff", 0)
        max_diff = details.get("max_abs_diff", 0)
        
        status = "‚úÖ" if match_rate == 1.0 else "‚ö†Ô∏è" if match_rate > 0.95 else "‚ùå"
        
        mean_str = f"{mean_diff:.6f}" if mean_diff is not None and not np.isnan(mean_diff) else "-"
        max_str = f"{max_diff:.6f}" if max_diff is not None and not np.isnan(max_diff) else "-"
        
        print(f"{status} {feat:<28} {match_rate:>11.2%} {mean_str:>12} {max_str:>12}")
    
    # Mismatches
    mismatches = report.get("mismatches", [])
    if mismatches:
        print("\n‚ö†Ô∏è Sample Mismatches (first 10):")
        print("-" * 70)
        for m in mismatches[:10]:
            print(f"   {m['feature']}: race={m['race_id']}, horse={m['horse_number']}")
            print(f"      JIT={m['jit_value']}, Train={m['train_value']}")
    
    # Sanity checks
    print("\nüîç Sanity Check - JIT Features:")
    if sanity_jit.get("issues"):
        for issue in sanity_jit["issues"][:5]:
            print(f"   ‚ö†Ô∏è {issue}")
    else:
        print("   ‚úÖ No critical issues found")
    
    print("\nüîç Sanity Check - Training Features:")
    if sanity_train.get("issues"):
        for issue in sanity_train["issues"][:5]:
            print(f"   ‚ö†Ô∏è {issue}")
    else:
        print("   ‚úÖ No critical issues found")
    
    print("\n" + "=" * 70)


def main():
    parser = argparse.ArgumentParser(description="Verify JIT vs Training Feature Consistency")
    parser.add_argument("--date", type=str, default="20241115", help="Target date YYYYMMDD")
    parser.add_argument("--output", type=str, default="reports/jit_verification_report.txt", help="Output report path")
    args = parser.parse_args()
    
    target_date = args.date
    logger.info(f"Starting verification for date: {target_date}")
    
    loader = JraVanDataLoader()
    
    # Generate features using both methods
    df_jit = generate_jit_features(loader, target_date, horse_ids=None)
    df_train = generate_training_features(loader, target_date)
    
    if df_jit.empty or df_train.empty:
        logger.error("Failed to generate features")
        return
    
    logger.info(f"JIT features: {len(df_jit)} records, {len(df_jit.columns)} columns")
    logger.info(f"Training features: {len(df_train)} records, {len(df_train.columns)} columns")
    
    # Compare features
    report = compare_features(df_jit, df_train)
    
    # Sanity checks
    sanity_jit = sanity_check_features(df_jit, "JIT")
    sanity_train = sanity_check_features(df_train, "Training")
    
    # Print report
    print_report(report, sanity_jit, sanity_train)
    
    # Save report
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    with open(args.output, 'w', encoding='utf-8') as f:
        import json
        f.write("# JIT vs Training Feature Verification Report\n")
        f.write(f"# Date: {target_date}\n")
        f.write(f"# Generated: {datetime.now().isoformat()}\n\n")
        f.write(json.dumps(report, indent=2, default=str))
    
    logger.info(f"Report saved to {args.output}")
    
    # Return exit code based on results
    if report.get("summary", {}).get("overall_match_rate", 0) < 0.99:
        logger.warning("Feature consistency issues detected!")
        return 1
    
    logger.info("‚úÖ Feature consistency verification passed!")
    return 0


if __name__ == "__main__":
    sys.exit(main() or 0)

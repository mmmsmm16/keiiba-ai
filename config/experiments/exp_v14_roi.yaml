experiment_name: "v14_roi"
description: "v14: ROI最適化モデル (期待値最大化損失)"

data:
  features: "v9_full"
  cache_path: "data/processed/lgbm_datasets_v9.pkl"
  train_years: [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]
  valid_year: 2025
  use_cache: true  # NARデータ含むため再生成
  jra_only: false   # NARも学習に含める
  target_type: "ranking"

model:
  type: "roi"  # ROIモデルタイプ
  
  roi_params:
    model_type: "simple"  # "simple" or "attention"
    loss_type: "evmax"    # "evmax", "odds_bce", "roi_proxy"
    hidden_dim: 1024        # 512 → 1024 に増加
    num_layers: 6           # 4 → 6層に増加
    dropout: 0.3
    epochs: 300             # 100 → 200 に増加 (Early Stoppingで早期終了可能)
    batch_size: 128
    lr: 0.0002              # 大きいモデルなので学習率を少し下げる
    patience: 30            # Early Stopping

evaluation:
  metric: "roi"
  strategies: ["tansho", "umaren", "wide", "sanrentan", "option_c"]
  jra_only: true
  
strategy:
  enabled: true
  min_roi: 100.0
  target_bet_types: ["tansho", "umaren", "wide"]
